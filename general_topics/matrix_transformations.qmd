---
title: "Matrix Transformations"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    html-math-method: katex
---

# Concepts and Questions on Matrix Transformations

## Properties for a Transformation to be a Linear Transformation
- **Reference**: Section 1.8
- **Definition**: A transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is linear if it satisfies two properties:
  1. **Additivity**: $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ for all vectors $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$.
  2. **Scalar Multiplication**: $T(c \mathbf{u}) = c T(\mathbf{u})$ for any scalar $c$ and vector $\mathbf{u} \in \mathbb{R}^n$.

- **Explanation**: These properties mean that linear transformations preserve the structure of vector spaces. For instance, a scaling transformation will uniformly stretch all vectors by the same factor, maintaining the direction of the vectors.
  
- **Example**: If $T(\mathbf{x}) = A \mathbf{x}$, where $A$ is a matrix, then $T$ is a linear transformation because matrix multiplication inherently satisfies both additivity and scalar multiplication.

## Definitions of "Onto" and "One-to-One" Transformations
- **Reference**: Sections 1.9 and 2.4
- **"Onto" Transformation (Surjective)**: A transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is "onto" if every vector in $\mathbb{R}^m$ is the image of at least one vector in $\mathbb{R}^n$. This implies that the range of $T$ spans $\mathbb{R}^m$.
  
- **"One-to-One" Transformation (Injective)**: A transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is "one-to-one" if different vectors in $\mathbb{R}^n$ map to different vectors in $\mathbb{R}^m$. This means that $T(\mathbf{u}) = T(\mathbf{v})$ implies $\mathbf{u} = \mathbf{v}$.
  
- **Example and Explanation**: If $T(\mathbf{x}) = A \mathbf{x}$ and $A$ is an $m \times n$ matrix:
  - **Onto**: $T$ is onto if the columns of $A$ span $\mathbb{R}^m$.
  - **One-to-One**: $T$ is one-to-one if the columns of $A$ are linearly independent, which holds if $A$ has a pivot in every column.

## Markov Chain and Steady State Solution
- **Reference**: Section 4.9
- **Definition**: A **Markov Chain** is a sequence of states where the probability of moving to the next state depends only on the current state. This process can be represented with a **transition matrix** $P$, where each entry $P_{ij}$ denotes the probability of transitioning from state $i$ to state $j$.

- **Steady State Solution**: The steady state of a Markov Chain is a vector $\mathbf{s}$ such that $P \mathbf{s} = \mathbf{s}$. This vector represents a stable distribution of probabilities across states that no longer changes after transitions.
  
- **Explanation**: To find the steady state vector $\mathbf{s}$, solve $(P - I) \mathbf{s} = 0$, ensuring that the sum of the entries in $\mathbf{s}$ is 1 (since they represent probabilities).

- **Example**: For a transition matrix
  $  P = \begin{bmatrix} 0.9 & 0.1 \\ 0.5 & 0.5 \end{bmatrix},
  \]
  the steady state vector $\mathbf{s} = \begin{bmatrix} s_1 \\ s_2 \end{bmatrix}$ satisfies:
  $  P \mathbf{s} = \mathbf{s}.
  \]
  Solving this system, we can find the steady state probabilities for long-term behavior.

---

# Summary of Matrix Transformations Concepts
- **Linear Transformation**: Defined by additivity and scalar multiplication, ensuring the transformation is consistent with vector space structure.
- **"Onto" and "One-to-One"**: Key properties for transformations, indicating whether every output has a corresponding input ("onto") and whether each input maps uniquely ("one-to-one").
- **Markov Chains and Steady State**: Used for modeling probabilistic systems that evolve over time, where the steady state represents stable, long-term probabilities.

This expanded section on matrix transformations connects linear algebra concepts with practical examples and applications, reinforcing understanding of these fundamental transformations.