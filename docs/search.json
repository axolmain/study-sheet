[
  {
    "objectID": "exam4/chapter6_2.html",
    "href": "exam4/chapter6_2.html",
    "title": "Chapter 6.2",
    "section": "",
    "text": "A set of vectors \\{u_1, \\ldots, u_p\\} is an orthogonal set if each pair of distinct vectors is orthogonal. Formally: \n\\langle u_i, u_j \\rangle = 0 \\quad \\text{whenever } i \\neq j.\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define orthogonal vectors\nv1 = np.array([3, 0])\nv2 = np.array([0, 4])\n\n# Plot vectors\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\nplt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Orthogonal Vectors')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nA set of vectors \\{u_1, \\ldots, u_p\\} is orthonormal if the vectors are pairwise orthogonal and have norm 1. Formally: \n\\langle e_j, e_k \\rangle =\n\\begin{cases}\n0 & \\text{if } j \\neq k, \\\\\n1 & \\text{if } j = k.\n\\end{cases}\n Layman’s Terms: A list of vectors that are all perpendicular to each other and have a length of 1.\n\n\n\nAn orthonormal basis is an orthonormal set of vectors that spans the vector space V. Every orthonormal list with \\dim V elements is an orthonormal basis. For example, the standard basis in \\mathbb{R}^n is an orthonormal basis.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Input vectors\nv1 = np.array([3, 1])\nv2 = np.array([2, 2])\n\n# Compute Gram-Schmidt\nu1 = v1\nu2 = v2 - np.dot(v2, u1) / np.dot(u1, u1) * u1\n\n# Normalize to create orthonormal vectors\ne1 = u1 / np.linalg.norm(u1)\ne2 = u2 / np.linalg.norm(u2)\n\n# Plot original and orthonormal vectors\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\nplt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\nplt.quiver(0, 0, e1[0], e1[1], angles='xy', scale_units='xy', scale=1, color='g', label='e1 (Orthonormal)')\nplt.quiver(0, 0, e2[0], e2[1], angles='xy', scale_units='xy', scale=1, color='purple', label='e2 (Orthonormal)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Gram-Schmidt Process')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTo find the projection of a vector v onto a vector u:\n\n\\text{proj}_u(v) = \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} u.\n\nProjections are useful in approximating vectors and decomposing spaces.\n\n\nShow the code\nimport numpy as np\n# Define vectors\nv = np.array([2, 3])\nu = np.array([4, 1])\n\n# Calculate projection of v onto u\nproj_u_v = (np.dot(v, u) / np.dot(u, u)) * u\n\n# Plot the original vectors\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')\n\n# Plot the projection using a black dashed line with increased thickness\nplt.plot([0, proj_u_v[0]], [0, proj_u_v[1]], 'k--', linewidth=2, label='proj_u(v)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Projection of v onto u')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nKey Fact: An m \\times n matrix U has orthonormal columns if and only if:\n\nU^T U = I.\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define an orthonormal matrix\nU = np.array([[1, 0], [0, 1]])\n\n# Define two example vectors\nx = np.array([3, 4])\ny = np.array([4, -3])\n\n# Apply transformation\nUx = U @ x\nUy = U @ y\n\n# Plot original vectors\nplt.quiver(0, 0, x[0], x[1], angles='xy', scale_units='xy', scale=1, color='r', label='x')\nplt.quiver(0, 0, y[0], y[1], angles='xy', scale_units='xy', scale=1, color='b', label='y')\n\n# Plot transformed vectors\nplt.quiver(0, 0, Ux[0], Ux[1], angles='xy', scale_units='xy', scale=1, color='g', label='Ux (Transformed)')\nplt.quiver(0, 0, Uy[0], Uy[1], angles='xy', scale_units='xy', scale=1, color='purple', label='Uy (Transformed)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.title('Orthonormal Transformation')\nplt.show()"
  },
  {
    "objectID": "exam4/chapter6_2.html#orthogonal-sets",
    "href": "exam4/chapter6_2.html#orthogonal-sets",
    "title": "Chapter 6.2",
    "section": "",
    "text": "A set of vectors \\{u_1, \\ldots, u_p\\} is an orthogonal set if each pair of distinct vectors is orthogonal. Formally: \n\\langle u_i, u_j \\rangle = 0 \\quad \\text{whenever } i \\neq j.\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define orthogonal vectors\nv1 = np.array([3, 0])\nv2 = np.array([0, 4])\n\n# Plot vectors\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\nplt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Orthogonal Vectors')\nplt.show()"
  },
  {
    "objectID": "exam4/chapter6_2.html#orthonormal-sets",
    "href": "exam4/chapter6_2.html#orthonormal-sets",
    "title": "Chapter 6.2",
    "section": "",
    "text": "A set of vectors \\{u_1, \\ldots, u_p\\} is orthonormal if the vectors are pairwise orthogonal and have norm 1. Formally: \n\\langle e_j, e_k \\rangle =\n\\begin{cases}\n0 & \\text{if } j \\neq k, \\\\\n1 & \\text{if } j = k.\n\\end{cases}\n Layman’s Terms: A list of vectors that are all perpendicular to each other and have a length of 1."
  },
  {
    "objectID": "exam4/chapter6_2.html#orthonormal-basis",
    "href": "exam4/chapter6_2.html#orthonormal-basis",
    "title": "Chapter 6.2",
    "section": "",
    "text": "An orthonormal basis is an orthonormal set of vectors that spans the vector space V. Every orthonormal list with \\dim V elements is an orthonormal basis. For example, the standard basis in \\mathbb{R}^n is an orthonormal basis.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Input vectors\nv1 = np.array([3, 1])\nv2 = np.array([2, 2])\n\n# Compute Gram-Schmidt\nu1 = v1\nu2 = v2 - np.dot(v2, u1) / np.dot(u1, u1) * u1\n\n# Normalize to create orthonormal vectors\ne1 = u1 / np.linalg.norm(u1)\ne2 = u2 / np.linalg.norm(u2)\n\n# Plot original and orthonormal vectors\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\nplt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\nplt.quiver(0, 0, e1[0], e1[1], angles='xy', scale_units='xy', scale=1, color='g', label='e1 (Orthonormal)')\nplt.quiver(0, 0, e2[0], e2[1], angles='xy', scale_units='xy', scale=1, color='purple', label='e2 (Orthonormal)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Gram-Schmidt Process')\nplt.show()"
  },
  {
    "objectID": "exam4/chapter6_2.html#projections-applications-of-orthogonality",
    "href": "exam4/chapter6_2.html#projections-applications-of-orthogonality",
    "title": "Chapter 6.2",
    "section": "",
    "text": "To find the projection of a vector v onto a vector u:\n\n\\text{proj}_u(v) = \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} u.\n\nProjections are useful in approximating vectors and decomposing spaces.\n\n\nShow the code\nimport numpy as np\n# Define vectors\nv = np.array([2, 3])\nu = np.array([4, 1])\n\n# Calculate projection of v onto u\nproj_u_v = (np.dot(v, u) / np.dot(u, u)) * u\n\n# Plot the original vectors\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='r', label='v')\nplt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1, color='b', label='u')\n\n# Plot the projection using a black dashed line with increased thickness\nplt.plot([0, proj_u_v[0]], [0, proj_u_v[1]], 'k--', linewidth=2, label='proj_u(v)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Projection of v onto u')\nplt.show()"
  },
  {
    "objectID": "exam4/chapter6_2.html#matrix-representation-of-orthonormal-sets",
    "href": "exam4/chapter6_2.html#matrix-representation-of-orthonormal-sets",
    "title": "Chapter 6.2",
    "section": "",
    "text": "Key Fact: An m \\times n matrix U has orthonormal columns if and only if:\n\nU^T U = I.\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define an orthonormal matrix\nU = np.array([[1, 0], [0, 1]])\n\n# Define two example vectors\nx = np.array([3, 4])\ny = np.array([4, -3])\n\n# Apply transformation\nUx = U @ x\nUy = U @ y\n\n# Plot original vectors\nplt.quiver(0, 0, x[0], x[1], angles='xy', scale_units='xy', scale=1, color='r', label='x')\nplt.quiver(0, 0, y[0], y[1], angles='xy', scale_units='xy', scale=1, color='b', label='y')\n\n# Plot transformed vectors\nplt.quiver(0, 0, Ux[0], Ux[1], angles='xy', scale_units='xy', scale=1, color='g', label='Ux (Transformed)')\nplt.quiver(0, 0, Uy[0], Uy[1], angles='xy', scale_units='xy', scale=1, color='purple', label='Uy (Transformed)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.title('Orthonormal Transformation')\nplt.show()"
  },
  {
    "objectID": "exam4/chapter6_2.html#proposition-norm-property",
    "href": "exam4/chapter6_2.html#proposition-norm-property",
    "title": "Chapter 6.2",
    "section": "3.1 Proposition: Norm Property",
    "text": "3.1 Proposition: Norm Property\nIf \\{e_1, \\ldots, e_m\\} is an orthonormal set, then for any scalars a_1, \\ldots, a_m: \n\\|a_1e_1 + \\ldots + a_me_m\\|^2 = |a_1|^2 + \\ldots + |a_m|^2.\n Why This Works: Orthogonality allows the Pythagorean theorem to hold, letting lengths add independently."
  },
  {
    "objectID": "exam4/chapter6_2.html#corollary",
    "href": "exam4/chapter6_2.html#corollary",
    "title": "Chapter 6.2",
    "section": "3.2 Corollary",
    "text": "3.2 Corollary\nEvery orthonormal set is linearly independent."
  },
  {
    "objectID": "exam4/chapter6_2.html#theorem-4-orthogonal-sets-are-independent",
    "href": "exam4/chapter6_2.html#theorem-4-orthogonal-sets-are-independent",
    "title": "Chapter 6.2",
    "section": "3.3 Theorem 4: Orthogonal Sets are Independent",
    "text": "3.3 Theorem 4: Orthogonal Sets are Independent\nAn orthogonal set \\{u_1, \\ldots, u_p\\} of nonzero vectors is linearly independent and forms a basis for its span.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define three vectors, two orthogonal and one linear combination\nv1 = np.array([2, 0])\nv2 = np.array([0, 3])\nv3 = 2 * v1 + v2  # Linearly dependent\n\n# Plot vectors\nplt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\nplt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\nplt.quiver(0, 0, v3[0], v3[1], angles='xy', scale_units='xy', scale=1, color='g', label='v3 (Dependent)')\n\n# Add details\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid()\nplt.legend()\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.title('Linear Dependence and Independence')\nplt.show()"
  },
  {
    "objectID": "exam4/chapter6_2.html#theorem-5-expansion-with-orthogonal-basis",
    "href": "exam4/chapter6_2.html#theorem-5-expansion-with-orthogonal-basis",
    "title": "Chapter 6.2",
    "section": "3.4 Theorem 5: Expansion with Orthogonal Basis",
    "text": "3.4 Theorem 5: Expansion with Orthogonal Basis\nFor an orthogonal basis \\{u_1, \\ldots, u_p\\}, any vector y in the span of the basis can be written as: \ny = c_1u_1 + \\cdots + c_pu_p, \\quad \\text{where } c_j = \\frac{\\langle y, u_j \\rangle}{\\langle u_j, u_j \\rangle}."
  },
  {
    "objectID": "exam4/chapter6_2.html#theorem-6-matrix-properties-of-orthonormal-columns",
    "href": "exam4/chapter6_2.html#theorem-6-matrix-properties-of-orthonormal-columns",
    "title": "Chapter 6.2",
    "section": "3.5 Theorem 6: Matrix Properties of Orthonormal Columns",
    "text": "3.5 Theorem 6: Matrix Properties of Orthonormal Columns\nFor a matrix U with orthonormal columns: - \\|Ux\\| = \\|x\\|, - (Ux) \\cdot (Uy) = x \\cdot y, - Orthogonality is preserved: (Ux) \\cdot (Uy) = 0 \\iff x \\cdot y = 0."
  },
  {
    "objectID": "exam4/chapter6_2.html#orthonormal-expansion-theorem",
    "href": "exam4/chapter6_2.html#orthonormal-expansion-theorem",
    "title": "Chapter 6.2",
    "section": "3.6 Orthonormal Expansion (Theorem)",
    "text": "3.6 Orthonormal Expansion (Theorem)\nIf \\{e_1, \\ldots, e_n\\} is an orthonormal basis of V, then for any vector v \\in V: \nv = \\langle v, e_1 \\rangle e_1 + \\cdots + \\langle v, e_n \\rangle e_n,\n and: \n\\|v\\|^2 = \\sum_{j=1}^n |\\langle v, e_j \\rangle|^2."
  },
  {
    "objectID": "exam4/chapter6_2.html#example-1-orthonormal-basis-in-mathbbr4",
    "href": "exam4/chapter6_2.html#example-1-orthonormal-basis-in-mathbbr4",
    "title": "Chapter 6.2",
    "section": "4.1 Example 1: Orthonormal Basis in \\mathbb{R}^4",
    "text": "4.1 Example 1: Orthonormal Basis in \\mathbb{R}^4\nConsider the set: \n\\left\\{\\left(\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}\\right), \\left(\\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, -\\frac{1}{2}\\right), \\left(\\frac{1}{2}, -\\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{2}\\right), \\left(-\\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{2}\\right)\\right\\}.\n This set is orthonormal and forms a basis of \\mathbb{R}^4."
  },
  {
    "objectID": "exam4/chapter6_2.html#example-2-orthogonal-projections",
    "href": "exam4/chapter6_2.html#example-2-orthogonal-projections",
    "title": "Chapter 6.2",
    "section": "4.2 Example 2: Orthogonal Projections",
    "text": "4.2 Example 2: Orthogonal Projections\nGiven vectors u = \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} and v = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, find \\text{proj}_u(v): \n\\text{proj}_u(v) = \\frac{\\langle v, u \\rangle}{\\langle u, u \\rangle} u."
  },
  {
    "objectID": "exam4/chapter6_3.html",
    "href": "exam4/chapter6_3.html",
    "title": "Chapter 6.3",
    "section": "",
    "text": "Definition (Orthogonal Projection):\nAn orthogonal projection onto a subspace W of an inner product space V is a linear transformation P: V \\to V such that:\n\nP^2 = P (idempotent property).\nP = P^*, i.e., P is self-adjoint (or Hermitian if over \\mathbb{C}), meaning \\langle P(x), y \\rangle = \\langle x, P(y) \\rangle.\n\nIn more intuitive terms, given a subspace W \\subseteq V, the orthogonal projection of a vector v \\in V onto W is the “closest point” in W to v. Formally, if w \\in W is chosen such that v - w is orthogonal to every vector in W, then P(v) = w.\nNotation:\nIf W is spanned by an orthonormal set \\{u_1, u_2, \\ldots, u_k\\}, the orthogonal projection of any v \\in V onto W is given by: \nP_W(v) = \\sum_{i=1}^{k} \\langle v, u_i \\rangle u_i.\n\n\n\nFor vectors u, v \\in V, the projection of u onto v is given by: \nP_v(u) = \\frac{\\langle u, v \\rangle}{\\|v\\|^2} v, \\quad \\text{provided } v \\neq 0."
  },
  {
    "objectID": "exam4/chapter6_3.html#orthogonal-projections",
    "href": "exam4/chapter6_3.html#orthogonal-projections",
    "title": "Chapter 6.3",
    "section": "",
    "text": "Definition (Orthogonal Projection):\nAn orthogonal projection onto a subspace W of an inner product space V is a linear transformation P: V \\to V such that:\n\nP^2 = P (idempotent property).\nP = P^*, i.e., P is self-adjoint (or Hermitian if over \\mathbb{C}), meaning \\langle P(x), y \\rangle = \\langle x, P(y) \\rangle.\n\nIn more intuitive terms, given a subspace W \\subseteq V, the orthogonal projection of a vector v \\in V onto W is the “closest point” in W to v. Formally, if w \\in W is chosen such that v - w is orthogonal to every vector in W, then P(v) = w.\nNotation:\nIf W is spanned by an orthonormal set \\{u_1, u_2, \\ldots, u_k\\}, the orthogonal projection of any v \\in V onto W is given by: \nP_W(v) = \\sum_{i=1}^{k} \\langle v, u_i \\rangle u_i.\n\n\n\nFor vectors u, v \\in V, the projection of u onto v is given by: \nP_v(u) = \\frac{\\langle u, v \\rangle}{\\|v\\|^2} v, \\quad \\text{provided } v \\neq 0."
  },
  {
    "objectID": "exam4/chapter6_3.html#theorem-1-characterization-of-orthogonal-projections",
    "href": "exam4/chapter6_3.html#theorem-1-characterization-of-orthogonal-projections",
    "title": "Chapter 6.3",
    "section": "3.1 Theorem 1: Characterization of Orthogonal Projections",
    "text": "3.1 Theorem 1: Characterization of Orthogonal Projections\nStatement:\nA linear operator P: V \\to V is an orthogonal projection onto some subspace W \\subseteq V if and only if P is idempotent (P^2 = P) and self-adjoint (P = P^*).\nProof Outline:\n1. (Necessity) If P is an orthogonal projection onto W, by definition P_W^2 = P_W and P_W is its own adjoint.\n2. (Sufficiency) If P is idempotent and self-adjoint, let W = \\text{Im}(P). For any v, P(v) \\in W and v - P(v) is orthogonal to W. Thus, P is the orthogonal projection onto W."
  },
  {
    "objectID": "exam4/chapter6_3.html#theorem-8-orthogonal-decomposition-theorem",
    "href": "exam4/chapter6_3.html#theorem-8-orthogonal-decomposition-theorem",
    "title": "Chapter 6.3",
    "section": "3.2 Theorem 8: Orthogonal Decomposition Theorem",
    "text": "3.2 Theorem 8: Orthogonal Decomposition Theorem\nStatement:\nFor any subspace W \\subseteq V, each y \\in V can be uniquely written as: \ny = \\hat{y} + z,\n where \\hat{y} \\in W and z \\in W^\\perp. If \\{u_1, \\ldots, u_p\\} is an orthogonal basis for W, then: \n\\hat{y} = \\frac{\\langle y, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1 + \\cdots + \\frac{\\langle y, u_p \\rangle}{\\langle u_p, u_p \\rangle} u_p, \\quad z = y - \\hat{y}.\n\nIf \\{e_1, \\ldots, e_m\\} is an orthonormal basis for W, then: \nP_W(y) = \\langle y, e_1 \\rangle e_1 + \\cdots + \\langle y, e_m \\rangle e_m."
  },
  {
    "objectID": "exam4/chapter6_3.html#theorem-9-best-approximation-theorem",
    "href": "exam4/chapter6_3.html#theorem-9-best-approximation-theorem",
    "title": "Chapter 6.3",
    "section": "3.3 Theorem 9: Best Approximation Theorem",
    "text": "3.3 Theorem 9: Best Approximation Theorem\nStatement:\nFor a subspace U \\subseteq V and v \\in V: \n\\|v - P_U(v)\\| \\leq \\|v - u\\| \\quad \\text{for all } u \\in U.\n Equality holds if and only if u = P_U(v). This underscores the idea that P_U(v) is the “closest” vector in U to v."
  },
  {
    "objectID": "exam4/chapter6_3.html#theorem-10-matrix-representation-of-orthogonal-projection",
    "href": "exam4/chapter6_3.html#theorem-10-matrix-representation-of-orthogonal-projection",
    "title": "Chapter 6.3",
    "section": "3.4 Theorem 10: Matrix Representation of Orthogonal Projection",
    "text": "3.4 Theorem 10: Matrix Representation of Orthogonal Projection\nStatement:\nIf \\{u_1, \\ldots, u_p\\} is an orthonormal basis for a subspace W \\subseteq \\mathbb{R}^n, and U = [u_1 \\cdots u_p], then for all y \\in \\mathbb{R}^n: \nP_W(y) = U U^T y.\n\nThis coincides with the matrix representation previously discussed."
  },
  {
    "objectID": "exam4/chapter6_3.html#proposition-1-matrix-representation-of-orthogonal-projections",
    "href": "exam4/chapter6_3.html#proposition-1-matrix-representation-of-orthogonal-projections",
    "title": "Chapter 6.3",
    "section": "3.5 Proposition 1: Matrix Representation of Orthogonal Projections",
    "text": "3.5 Proposition 1: Matrix Representation of Orthogonal Projections\nStatement:\nIf U = [u_1 \\ u_2 \\ \\cdots \\ u_k] is a matrix whose columns form an orthonormal basis for W \\subseteq \\mathbb{R}^n, then the orthogonal projection onto W is given by: \nP_W = U U^T.\n\nExplanation:\nSince each u_i is orthonormal, U^T U = I_k. Then P_W acts on a vector v as: \nP_W(v) = U (U^T v) = \\sum_{i=1}^{k} \\langle v, u_i \\rangle u_i.\n\nProof Sketch:\n- Idempotent: (U U^T)(U U^T) = U (U^T U) U^T = U I_k U^T = U U^T. - Symmetric: (U U^T)^T = U U^T.\nThus, P_W represents an orthogonal projection."
  },
  {
    "objectID": "exam4/chapter6_3.html#example-1-orthogonal-projection-onto-a-given-line-in-mathbbr2",
    "href": "exam4/chapter6_3.html#example-1-orthogonal-projection-onto-a-given-line-in-mathbbr2",
    "title": "Chapter 6.3",
    "section": "4.1 Example 1: Orthogonal Projection onto a Given Line in \\mathbb{R}^2",
    "text": "4.1 Example 1: Orthogonal Projection onto a Given Line in \\mathbb{R}^2\nSet-up:\nLet V = \\mathbb{R}^2 and consider W = \\text{span}\\{u\\} where u = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}. For v = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, find P_W(v).\nSolution Steps: 1. Since u is normalized, P_W(v) = \\langle v, u \\rangle u. 2. Compute \\langle v, u \\rangle = [2 \\ 1]\\begin{bmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{bmatrix} = \\frac{3}{\\sqrt{2}}. 3. Therefore: \n   P_W(v) = \\frac{3}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{3}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\end{bmatrix}.\n   \nPython Computation:\n\n\nShow the code\nimport numpy as np\n\nv = np.array([2, 1])\nu = (1/np.sqrt(2)) * np.array([1, 1])\nproj_v_on_u = (v @ u) * u\nproj_v_on_u\n\n\narray([1.5, 1.5])"
  },
  {
    "objectID": "exam4/chapter6_3.html#example-2-orthogonal-projection-onto-a-2d-subspace-in-mathbbr3",
    "href": "exam4/chapter6_3.html#example-2-orthogonal-projection-onto-a-2d-subspace-in-mathbbr3",
    "title": "Chapter 6.3",
    "section": "4.2 Example 2: Orthogonal Projection onto a 2D Subspace in \\mathbb{R}^3",
    "text": "4.2 Example 2: Orthogonal Projection onto a 2D Subspace in \\mathbb{R}^3\nSet-up:\nLet W = \\text{span}\\{u_1, u_2\\} in \\mathbb{R}^3 where: \nu_1 = \\frac{1}{\\sqrt{5}}\\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}, \\quad\nu_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}.\n For v = \\begin{bmatrix} 3 \\\\ 3 \\\\ 1 \\end{bmatrix}, find P_W(v).\nSolution Steps: 1. Compute \\langle v, u_1 \\rangle and \\langle v, u_2 \\rangle. 2. Then: \n   P_W(v) = \\langle v, u_1 \\rangle u_1 + \\langle v, u_2 \\rangle u_2.\n   \nPython Computation:\n\n\nShow the code\nv = np.array([3, 3, 1])\nu1 = (1/np.sqrt(5)) * np.array([1, 2, 0])\nu2 = (1/np.sqrt(2)) * np.array([0, 1, 1])\n\ncoeff_u1 = v @ u1\ncoeff_u2 = v @ u2\n\nP_v = coeff_u1 * u1 + coeff_u2 * u2\nP_v\n\n\narray([1.8, 5.6, 2. ])"
  },
  {
    "objectID": "exam4/chapter6_5.html",
    "href": "exam4/chapter6_5.html",
    "title": "Chapter 6.5",
    "section": "",
    "text": "If A is m \\times n and b is in \\mathbb{R}^m, a least-squares solution of A\\mathbf{x}=\\mathbf{b} is an \\hat{\\mathbf{x}} in \\mathbb{R}^n such that:\n\n\\|\\mathbf{b}-A\\hat{\\mathbf{x}}\\| \\le \\|\\mathbf{b} - A\\mathbf{x}\\|\n\nfor all x in \\mathbb{R}^n.\n\n\n\n\n\n\n\n\n\n\n\n\nGiven a system of linear equations A\\mathbf{x} = \\mathbf{b}, the normal equations are given by:\n\nA^TA\\mathbf{x} = A^T\\mathbf{b}\n\nwhere A^T is the transpose of matrix A."
  },
  {
    "objectID": "exam4/chapter6_5.html#least-squares-problems",
    "href": "exam4/chapter6_5.html#least-squares-problems",
    "title": "Chapter 6.5",
    "section": "",
    "text": "If A is m \\times n and b is in \\mathbb{R}^m, a least-squares solution of A\\mathbf{x}=\\mathbf{b} is an \\hat{\\mathbf{x}} in \\mathbb{R}^n such that:\n\n\\|\\mathbf{b}-A\\hat{\\mathbf{x}}\\| \\le \\|\\mathbf{b} - A\\mathbf{x}\\|\n\nfor all x in \\mathbb{R}^n."
  },
  {
    "objectID": "exam4/chapter6_5.html#normal-equations",
    "href": "exam4/chapter6_5.html#normal-equations",
    "title": "Chapter 6.5",
    "section": "",
    "text": "Given a system of linear equations A\\mathbf{x} = \\mathbf{b}, the normal equations are given by:\n\nA^TA\\mathbf{x} = A^T\\mathbf{b}\n\nwhere A^T is the transpose of matrix A."
  },
  {
    "objectID": "exam4/chapter6_5.html#theorem-13-the-least-squares-theorem",
    "href": "exam4/chapter6_5.html#theorem-13-the-least-squares-theorem",
    "title": "Chapter 6.5",
    "section": "2.1 Theorem 13: The Least-Squares Theorem",
    "text": "2.1 Theorem 13: The Least-Squares Theorem\n\n\n\n\n\n\nStatement: The set of least-squares solutions of A\\mathbf{x} = \\mathbf{b} coincides with the nonempty set of solutions of the normal equations A^TA\\mathbf{x} = A^T\\mathbf{b}.\nLayman’s Explanation: Imagine you have a system of linear equations, A\\mathbf{x} = \\mathbf{b}, that might not have an exact solution. This often happens when you have more equations than unknowns (an overdetermined system). In such cases, you can’t find an \\mathbf{x} that perfectly satisfies the equation.\nThe “least-squares solution” is the best approximation you can get. It’s the \\mathbf{x} that minimizes the distance between A\\mathbf{x} and \\mathbf{b}. Think of it like trying to fit a line to a set of points that don’t perfectly align. The least-squares line is the one that comes closest to all the points, minimizing the overall error.\nThis theorem says that finding this best approximation (the least-squares solution) is the same as solving a different set of equations called the “normal equations,” which are given by A^TA\\mathbf{x} = A^T\\mathbf{b}. Importantly, the normal equations always have at least one solution, even if the original system doesn’t.\nExample: Let’s say you’re trying to find the best-fit line y = mx + c for the following data points: (1, 2), (2, 3), (3, 5).\nYou can represent this as a system of equations:\n\n\\begin{aligned}\nm + c &= 2 \\\\\n2m + c &= 3 \\\\\n3m + c &= 5\n\\end{aligned}\n\nThis can be written in matrix form as A\\mathbf{x} = \\mathbf{b}, where:\nA = \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix}, \\mathbf{x} = \\begin{bmatrix} m \\\\ c \\end{bmatrix}, and \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}.\nThis system has no exact solution (the points don’t lie on a perfect line). To find the least-squares solution (the best-fit line), you would solve the normal equations:\nA^TA\\mathbf{x} = A^T\\mathbf{b}\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix} \\begin{bmatrix} m \\\\ c \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}\nSolving this system will give you the values of m and c that define the best-fit line.\nProof Explanation: The proof has two main parts:\n\nShowing that a least-squares solution satisfies the normal equations: It’s already been shown (in a previous part of the text, referred to as “above”) that if \\hat{\\mathbf{x}} is a least-squares solution, then it must satisfy the normal equations A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}.\nShowing that a solution to the normal equations is a least-squares solution: Now, let’s assume \\hat{\\mathbf{x}} satisfies the normal equations: A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}.\n\nThis implies that A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = \\mathbf{0}.\nThis means that the vector \\mathbf{b} - A\\hat{\\mathbf{x}} is orthogonal (perpendicular) to the rows of A^T, and therefore orthogonal to the columns of A.\nSince the columns of A span the column space of A (denoted as Col A), the vector \\mathbf{b} - A\\hat{\\mathbf{x}} is orthogonal to every vector in Col A.\nWe can decompose \\mathbf{b} into two parts: \\mathbf{b} = A\\hat{\\mathbf{x}} + (\\mathbf{b} - A\\hat{\\mathbf{x}}). The first part, A\\hat{\\mathbf{x}}, lies in Col A, and the second part, (\\mathbf{b} - A\\hat{\\mathbf{x}}), is orthogonal to Col A.\nBy the Pythagorean Theorem, \\|\\mathbf{b}\\|^2 = \\|A\\hat{\\mathbf{x}}\\|^2 + \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|^2.\nFor any other vector \\mathbf{x} in \\mathbb{R}^n, \\mathbf{b} - A\\mathbf{x} can also be decomposed into a part in Col A and a part orthogonal to Col A.\nAgain, by the Pythagorean Theorem, \\|\\mathbf{b}\\|^2 = \\|A\\mathbf{x}\\|^2 + \\|\\mathbf{b} - A\\mathbf{x}\\|^2.\nSince \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|^2 is the squared distance from \\mathbf{b} to Col A, and this distance is minimized when \\mathbf{x} = \\hat{\\mathbf{x}}, we have \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|^2 \\le \\|\\mathbf{b} - A\\mathbf{x}\\|^2.\nTherefore, \\hat{\\mathbf{x}} is a least-squares solution."
  },
  {
    "objectID": "exam4/chapter6_5.html#theorem-14",
    "href": "exam4/chapter6_5.html#theorem-14",
    "title": "Chapter 6.5",
    "section": "2.2 Theorem 14",
    "text": "2.2 Theorem 14\n\n\n\n\n\n\nLet A be an m \\times n matrix. The following statements are logically equivalent:\n\nThe equation A\\mathbf{x} = \\mathbf{b} has a unique least-squares solution for each \\mathbf{b} \\in \\mathbb{R}^m.\nThe columns of A are linearly independent.\nThe matrix A^T A is invertible.\n\nWhen these statements are true, the least-squares solution \\hat{\\mathbf{x}} is given by:\n\n\\hat{\\mathbf{x}} = (A^T A)^{-1} A^T \\mathbf{b}"
  },
  {
    "objectID": "exam4/chapter6_5.html#theorem-15",
    "href": "exam4/chapter6_5.html#theorem-15",
    "title": "Chapter 6.5",
    "section": "2.3 Theorem 15",
    "text": "2.3 Theorem 15\n\n\n\n\n\n\nGiven an m \\times n matrix A with linearly independent columns, let A = QR be a QR factorization of A (as defined in Theorem 12). For every \\mathbf{b} in \\mathbb{R}^m, the equation A\\mathbf{x} = \\mathbf{b} has a unique least-squares solution, which is given by:\n\n\\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}\n\nProof (in simple terms)\n\nStart with the formula for \\hat{\\mathbf{x}}: \n\\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}\n\nSubstitute A = QR into the equation A\\mathbf{x}: \nA\\hat{\\mathbf{x}} = QR\\hat{\\mathbf{x}}\n\nReplace \\hat{\\mathbf{x}} with R^{-1}Q^T\\mathbf{b}: \nA\\hat{\\mathbf{x}} = QRR^{-1}Q^T\\mathbf{b}\n\nSimplify RR^{-1} (which equals the identity matrix I): \nA\\hat{\\mathbf{x}} = QQ^T\\mathbf{b}\n\nBy Theorem 12, the columns of Q form an orthonormal basis for the column space of A. Therefore, QQ^T\\mathbf{b} is the projection of \\mathbf{b} onto the column space of A.\nThis shows that \\hat{\\mathbf{x}} minimizes the error (i.e., the difference between A\\mathbf{x} and \\mathbf{b}), meaning \\hat{\\mathbf{x}} is the least-squares solution.\nSince R is invertible and Q is orthonormal, \\hat{\\mathbf{x}} is unique.\n\nNumerical Note\n\nSince R is an upper triangular matrix, it’s computationally efficient to solve the equation: \nR\\mathbf{x} = Q^T\\mathbf{b}\n\nThis avoids directly calculating R^{-1}, which can be slow and error-prone. Instead, use back-substitution or row operations to solve for \\mathbf{x} efficiently."
  },
  {
    "objectID": "exam4/chapter6_5.html#example-1-finding-the-least-squares-solution",
    "href": "exam4/chapter6_5.html#example-1-finding-the-least-squares-solution",
    "title": "Chapter 6.5",
    "section": "3.1 Example 1: Finding the Least-Squares Solution",
    "text": "3.1 Example 1: Finding the Least-Squares Solution\nFind the least-squares solution of the inconsistent system A\\mathbf{x} = \\mathbf{b} for\nA = \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}.\n\n3.1.1 Solution\n\nCalculate A^TA and A^T\\mathbf{b}:\nA^TA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix} = \\begin{bmatrix} 14 & 6 \\\\ 6 & 3 \\end{bmatrix}\nA^T\\mathbf{b} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 23 \\\\ 10 \\end{bmatrix}\nSolve the normal equations A^TA\\mathbf{x} = A^T\\mathbf{b}:\n\\begin{bmatrix} 14 & 6 \\\\ 6 & 3 \\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 23 \\\\ 10 \\end{bmatrix}\nSolving this system gives \\mathbf{x} = \\begin{bmatrix} 1.3 \\\\ 0.4 \\end{bmatrix}.\n\n\n\nShow the code\nimport numpy as np\n\n# Define A and b\nA = np.array([[1, 1], [2, 1], [3, 1]])\nb = np.array([2, 3, 5])\n\n# Calculate A^T * A and A^T * b\nATA = A.T @ A\nATb = A.T @ b\n\n# Solve the normal equations\nx = np.linalg.solve(ATA, ATb)\n\nprint(\"Least-squares solution:\", x)\n\n\nLeast-squares solution: [1.5        0.33333333]"
  },
  {
    "objectID": "exam4/chapter6_5.html#example-2-using-qr-factorization",
    "href": "exam4/chapter6_5.html#example-2-using-qr-factorization",
    "title": "Chapter 6.5",
    "section": "3.2 Example 2: Using QR Factorization",
    "text": "3.2 Example 2: Using QR Factorization\nFind the least-squares solution of the system in Example 1 using a QR factorization of A.\n\n3.2.1 Solution\n\nFind the QR factorization of A:\nUsing the Gram-Schmidt process (or a software package), we can find A = QR, where\nQ = \\begin{bmatrix} 1/\\sqrt{14} & 1/\\sqrt{3} \\\\ 2/\\sqrt{14} & 0 \\\\ 3/\\sqrt{14} & -1/\\sqrt{3} \\end{bmatrix}, R = \\begin{bmatrix} \\sqrt{14} & \\sqrt{14}/\\sqrt{3} \\\\ 0 & \\sqrt{3} \\end{bmatrix}.\nCalculate Q^T\\mathbf{b}:\nQ^T\\mathbf{b} = \\begin{bmatrix} 1/\\sqrt{14} & 2/\\sqrt{14} & 3/\\sqrt{14} \\\\ 1/\\sqrt{3} & 0 & -1/\\sqrt{3} \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 23/\\sqrt{14} \\\\ -3/\\sqrt{3} \\end{bmatrix}\nSolve R\\mathbf{x} = Q^T\\mathbf{b}:\n\\begin{bmatrix} \\sqrt{14} & \\sqrt{14}/\\sqrt{3} \\\\ 0 & \\sqrt{3} \\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 23/\\sqrt{14} \\\\ -3/\\sqrt{3} \\end{bmatrix}\nSolving this system (using back-substitution) gives \\mathbf{x} = \\begin{bmatrix} 1.3 \\\\ 0.4 \\end{bmatrix}, which matches the solution from Example 1.\n\n\n\nShow the code\nimport numpy as np\n\n# Define A and b\nA = np.array([[1, 1], [2, 1], [3, 1]])\nb = np.array([2, 3, 5])\n\n# Calculate QR factorization\nQ, R = np.linalg.qr(A)\n\n# Calculate Q^T * b\nQTb = Q.T @ b\n\n# Solve Rx = Q^T * b\nx = np.linalg.solve(R, QTb)\n\nprint(\"Least-squares solution:\", x)\n\n\nLeast-squares solution: [1.5        0.33333333]"
  },
  {
    "objectID": "exam4/chapter6_5.html#visualizing-the-least-squares-solution",
    "href": "exam4/chapter6_5.html#visualizing-the-least-squares-solution",
    "title": "Chapter 6.5",
    "section": "4.1 Visualizing the Least-Squares Solution",
    "text": "4.1 Visualizing the Least-Squares Solution\nThe following plot shows the data points from Example 1 and the least-squares line:\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data points\nx_data = np.array([1, 2, 3])\ny_data = np.array([2, 3, 5])\n\n# Least-squares solution from Example 1\nm = 1.3\nc = 0.4\n\n# Generate points for the line\nx_line = np.linspace(0, 4, 100)\ny_line = m * x_line + c\n\n# Plot the data and the line\nplt.scatter(x_data, y_data, label=\"Data points\")\nplt.plot(x_line, y_line, label=\"Least-squares line\", color=\"red\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Least-Squares Line\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nExplanation:\n\nThe blue dots represent the original data points.\nThe red line is the least-squares line, which is the best-fit line that minimizes the sum of the squared vertical distances between the points and the line."
  },
  {
    "objectID": "general_topics/vector_spaces_and_dimensions.html",
    "href": "general_topics/vector_spaces_and_dimensions.html",
    "title": "Vector Spaces and Dimension",
    "section": "",
    "text": "Note\n\n\n\nA vector space V is finite-dimensional if it can be spanned by a finite set of vectors. The dimension of V, denoted \\dim V, is the number of vectors in any basis for V.\n\n\n\n\nThe standard basis for \\mathbb{R}^3 consists of: \n\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n Therefore, \\dim \\mathbb{R}^3 = 3\n\n\n\n\n\nZero vector space: \\dim \\{\\mathbf{0}\\} = 0\nInfinite-dimensional spaces: The space of all polynomials P has no finite basis\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe definition of Span: The span of a set of vectors is the set of all linear combinations of those vectors. For a set of vectors \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}, the span is denoted \\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition of Linear Independence and Dependence: A set of vectors is linearly independent if no vector in the set is a linear combination of the others; otherwise, the set is linearly dependent.\n\n\n\n\n\n\n\n\nHomogeneous System Form\n\n\n\nA homogeneous system has the form A\\mathbf{x} = \\mathbf{0}\n\n\n\n\n\n\\begin{aligned}\n2x + 3y - z &= 0 \\\\\n4x - 2y + 2z &= 0\n\\end{aligned}\n\nIn matrix form: \n\\begin{bmatrix}\n2 & 3 & -1 \\\\\n4 & -2 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix}\n= \\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\nTheorem 9\n\n\n\nIf a vector space V has a basis \\mathcal{B} = \\{ \\mathbf{b}_1, \\dots, \\mathbf{b}_n \\}, then any set in V containing more than n vectors must be linearly dependent.\n\n\n\n\n\n\n\n\nTheorem 10\n\n\n\nIf a vector space V has a basis of n vectors, then every basis of V must consist of exactly n vectors.\n\n\n\n\n\n\n\n\nTheorem 11\n\n\n\nLet H be a subspace of a finite-dimensional vector space V. Any linearly independent set in H can be expanded, if necessary, to a basis for H. Also, H is finite-dimensional and\n\\dim H \\leq \\dim V\n\n\n\n\n\n\n\n\nThe Basis Theorem (Theorem 12)\n\n\n\nLet V be a p-dimensional vector space, p \\geq 1. Any linearly independent set of exactly p elements in V is automatically a basis for V.\n\n\n\n\n\n\n\n\nSolve the system: \n\\begin{aligned}\nx + y + z &= 0 \\\\\n2x + 2y + 2z &= 0\n\\end{aligned}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNotice the second equation is a multiple of the first\nSystem reduces to x + y + z = 0\nGeneral solution: \\begin{bmatrix} x \\\\ y \\\\ -(x+y) \\end{bmatrix} for any x, y\nBasis: \\text{span}\\left(\\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\right)\n\n\n\n\n\n\n\nExplain why the standard basis for \\mathbb{R}^3 has exactly three vectors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe space \\mathbb{R}^3 requires three linearly independent vectors to span it fully. Since the standard basis \\{\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\} spans \\mathbb{R}^3 and no subset of these vectors can do so, they form a basis, making \\dim \\mathbb{R}^3 = 3.\n\n\n\n\n\n\nFind a basis for the null space of: \nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nRow reduce to: \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix}\nFree variables: y, z\nExpress x: x = -2y - 3z\nBasis: \\left\\{\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -3 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}"
  },
  {
    "objectID": "general_topics/vector_spaces_and_dimensions.html#key-concepts-and-definitions",
    "href": "general_topics/vector_spaces_and_dimensions.html#key-concepts-and-definitions",
    "title": "Vector Spaces and Dimension",
    "section": "",
    "text": "Note\n\n\n\nThe definition of Span: The span of a set of vectors is the set of all linear combinations of those vectors. For a set of vectors \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}, the span is denoted \\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition of Linear Independence and Dependence: A set of vectors is linearly independent if no vector in the set is a linear combination of the others; otherwise, the set is linearly dependent.\n\n\n\n\n\n\n\n\nHomogeneous System Form\n\n\n\nA homogeneous system has the form A\\mathbf{x} = \\mathbf{0}\n\n\n\n\n\n\\begin{aligned}\n2x + 3y - z &= 0 \\\\\n4x - 2y + 2z &= 0\n\\end{aligned}\n\nIn matrix form: \n\\begin{bmatrix}\n2 & 3 & -1 \\\\\n4 & -2 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix}\n= \\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\nTheorem 9\n\n\n\nIf a vector space V has a basis \\mathcal{B} = \\{ \\mathbf{b}_1, \\dots, \\mathbf{b}_n \\}, then any set in V containing more than n vectors must be linearly dependent.\n\n\n\n\n\n\n\n\nTheorem 10\n\n\n\nIf a vector space V has a basis of n vectors, then every basis of V must consist of exactly n vectors.\n\n\n\n\n\n\n\n\nTheorem 11\n\n\n\nLet H be a subspace of a finite-dimensional vector space V. Any linearly independent set in H can be expanded, if necessary, to a basis for H. Also, H is finite-dimensional and\n\\dim H \\leq \\dim V\n\n\n\n\n\n\n\n\nThe Basis Theorem (Theorem 12)\n\n\n\nLet V be a p-dimensional vector space, p \\geq 1. Any linearly independent set of exactly p elements in V is automatically a basis for V."
  },
  {
    "objectID": "general_topics/vector_spaces_and_dimensions.html#practice-problems",
    "href": "general_topics/vector_spaces_and_dimensions.html#practice-problems",
    "title": "Vector Spaces and Dimension",
    "section": "",
    "text": "Solve the system: \n\\begin{aligned}\nx + y + z &= 0 \\\\\n2x + 2y + 2z &= 0\n\\end{aligned}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNotice the second equation is a multiple of the first\nSystem reduces to x + y + z = 0\nGeneral solution: \\begin{bmatrix} x \\\\ y \\\\ -(x+y) \\end{bmatrix} for any x, y\nBasis: \\text{span}\\left(\\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\right)\n\n\n\n\n\n\n\nExplain why the standard basis for \\mathbb{R}^3 has exactly three vectors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe space \\mathbb{R}^3 requires three linearly independent vectors to span it fully. Since the standard basis \\{\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\} spans \\mathbb{R}^3 and no subset of these vectors can do so, they form a basis, making \\dim \\mathbb{R}^3 = 3.\n\n\n\n\n\n\nFind a basis for the null space of: \nA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nRow reduce to: \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{bmatrix}\nFree variables: y, z\nExpress x: x = -2y - 3z\nBasis: \\left\\{\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -3 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}"
  },
  {
    "objectID": "general_topics/predicting_the_future.html",
    "href": "general_topics/predicting_the_future.html",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "The intersection of probability theory and linear algebra provides powerful tools for analyzing stochastic processes and Markov chains. This study guide explores how vectors and matrices can represent probabilistic systems and help us understand their behavior over time.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A probability vector is a column vector p = [p_1, p_2, ..., p_n]^T where:\n\nAll components are non-negative: p_i \\geq 0 for all i\nThe sum of all components equals 1: \\sum_{i=1}^n p_i = 1\n\nThese vectors represent probability distributions over a finite set of states.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A stochastic matrix (or probability matrix) P is a square matrix where:\n\nAll entries are non-negative: p_{ij} \\geq 0 for all i,j\nEach row sums to 1: \\sum_{j=1}^n p_{ij} = 1 for all i\n\nThe entry p_{ij} represents the probability of transitioning from state i to state j.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A steady state vector (or stationary distribution) q is a probability vector that satisfies: q^T P = q^T or Pq = q\nThis vector represents the long-term probabilities of being in each state, regardless of the initial state.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A Markov chain is a sequence of random variables \\{X_n\\} where:\n\nThe probability of the next state depends only on the current state (Markov property)\nThese probabilities are represented by a stochastic matrix P\nThe state probabilities at time n are given by x_n = P^n x_0, where x_0 is the initial distribution\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A regular stochastic matrix is a stochastic matrix P that satisfies the following condition:\n\nThere exists a positive integer k such that all entries of P^k are strictly positive: (P^k)_{ij} &gt; 0 for all i, j.\n\nThis property ensures that, regardless of the initial state, the Markov chain associated with P will converge to a unique steady-state vector as n \\to \\infty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: Two matrices A and B are similar if there exists an invertible matrix P such that B = P^{-1}AP. Similar matrices share important properties like rank, determinant, and eigenvalues.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For stochastic matrices:\n\nThe largest eigenvalue is always 1\nAll eigenvalues have absolute value ≤ 1\nThe steady state vector is an eigenvector corresponding to eigenvalue 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf P is an n \\times n regular stochastic matrix, then P has a unique steady-state vector q. Further, if x_0 is any initial state and x_{k + 1} = Px_k for k = 0, 1, 2, \\dots, then the Markov chain \\{x_k\\} converges to q as k \\rightarrow \\infty.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a positive stochastic matrix:\n\nThe spectral radius (largest eigenvalue) equals 1\nThe corresponding eigenvector has all positive entries\nThis eigenvalue-eigenvector pair is unique\n\n\n\n\n\n\n\n\n\nThe PageRank algorithm uses a modified stochastic matrix to rank web pages. The steady state vector gives the importance scores of pages.\n\n\n\nLeslie matrices (a type of stochastic matrix) model population dynamics in age-structured populations.\n\n\n\nInput-output models in economics use stochastic matrices to represent flows between economic sectors.\n\n\n\n\n\n\nGiven the stochastic matrix: P = \\begin{bmatrix}\n0.7 & 0.3 \\\\\n0.4 & 0.6\n\\end{bmatrix} Find its steady state vector.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nLet q = [x, y]^T be the steady state vector\nWrite Pq = q: \\begin{bmatrix} 0.7x + 0.3y = x \\\\ 0.4x + 0.6y = y \\end{bmatrix}\nUse x + y = 1\nSolve to get q = [0.571, 0.429]^T\n\n\n\n\n\n\n\nShow that if P is stochastic, then P^n is also stochastic for any positive integer n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nUse induction on n\nBase case: True for n=1\nInductive step: Show if true for k, then true for k+1\nUse properties of matrix multiplication\n\n\n\n\n\n\n\nFor a regular stochastic matrix, prove that 1 is an eigenvalue and all other eigenvalues have absolute value less than 1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nShow that (1,v) is an eigenvalue-eigenvector pair where v is column of ones\nUse Gershgorin’s circle theorem to bound other eigenvalues\nUse regularity to show strict inequality\n\n\n\n\n\n\n\n\n\n3Blue1Brown - Essence of Linear Algebra (YouTube)\nGilbert Strang’s Linear Algebra Lectures (MIT OpenCourseWare)\n“Introduction to Probability” by Bertsekas and Tsitsiklis\n“Matrix Analysis” by Horn and Johnson\n\n\n\n\n\nForgetting to verify the stochastic property when computing matrix powers\nMisinterpreting steady state probabilities as transition probabilities\nAssuming all stochastic matrices have unique steady states (only true for regular matrices)\nNeglecting to normalize probability vectors after computations"
  },
  {
    "objectID": "general_topics/predicting_the_future.html#introduction",
    "href": "general_topics/predicting_the_future.html#introduction",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "The intersection of probability theory and linear algebra provides powerful tools for analyzing stochastic processes and Markov chains. This study guide explores how vectors and matrices can represent probabilistic systems and help us understand their behavior over time."
  },
  {
    "objectID": "general_topics/predicting_the_future.html#key-concepts-and-definitions",
    "href": "general_topics/predicting_the_future.html#key-concepts-and-definitions",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "Note\n\n\n\nDefinition: A probability vector is a column vector p = [p_1, p_2, ..., p_n]^T where:\n\nAll components are non-negative: p_i \\geq 0 for all i\nThe sum of all components equals 1: \\sum_{i=1}^n p_i = 1\n\nThese vectors represent probability distributions over a finite set of states.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A stochastic matrix (or probability matrix) P is a square matrix where:\n\nAll entries are non-negative: p_{ij} \\geq 0 for all i,j\nEach row sums to 1: \\sum_{j=1}^n p_{ij} = 1 for all i\n\nThe entry p_{ij} represents the probability of transitioning from state i to state j.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A steady state vector (or stationary distribution) q is a probability vector that satisfies: q^T P = q^T or Pq = q\nThis vector represents the long-term probabilities of being in each state, regardless of the initial state.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A Markov chain is a sequence of random variables \\{X_n\\} where:\n\nThe probability of the next state depends only on the current state (Markov property)\nThese probabilities are represented by a stochastic matrix P\nThe state probabilities at time n are given by x_n = P^n x_0, where x_0 is the initial distribution\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A regular stochastic matrix is a stochastic matrix P that satisfies the following condition:\n\nThere exists a positive integer k such that all entries of P^k are strictly positive: (P^k)_{ij} &gt; 0 for all i, j.\n\nThis property ensures that, regardless of the initial state, the Markov chain associated with P will converge to a unique steady-state vector as n \\to \\infty."
  },
  {
    "objectID": "general_topics/predicting_the_future.html#relationship-to-other-linear-algebra-topics",
    "href": "general_topics/predicting_the_future.html#relationship-to-other-linear-algebra-topics",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "Note\n\n\n\nDefinition: Two matrices A and B are similar if there exists an invertible matrix P such that B = P^{-1}AP. Similar matrices share important properties like rank, determinant, and eigenvalues.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For stochastic matrices:\n\nThe largest eigenvalue is always 1\nAll eigenvalues have absolute value ≤ 1\nThe steady state vector is an eigenvector corresponding to eigenvalue 1"
  },
  {
    "objectID": "general_topics/predicting_the_future.html#important-theorems",
    "href": "general_topics/predicting_the_future.html#important-theorems",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "Tip\n\n\n\nIf P is an n \\times n regular stochastic matrix, then P has a unique steady-state vector q. Further, if x_0 is any initial state and x_{k + 1} = Px_k for k = 0, 1, 2, \\dots, then the Markov chain \\{x_k\\} converges to q as k \\rightarrow \\infty.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a positive stochastic matrix:\n\nThe spectral radius (largest eigenvalue) equals 1\nThe corresponding eigenvector has all positive entries\nThis eigenvalue-eigenvector pair is unique"
  },
  {
    "objectID": "general_topics/predicting_the_future.html#special-cases-or-applications",
    "href": "general_topics/predicting_the_future.html#special-cases-or-applications",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "The PageRank algorithm uses a modified stochastic matrix to rank web pages. The steady state vector gives the importance scores of pages.\n\n\n\nLeslie matrices (a type of stochastic matrix) model population dynamics in age-structured populations.\n\n\n\nInput-output models in economics use stochastic matrices to represent flows between economic sectors."
  },
  {
    "objectID": "general_topics/predicting_the_future.html#practice-problems",
    "href": "general_topics/predicting_the_future.html#practice-problems",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "Given the stochastic matrix: P = \\begin{bmatrix}\n0.7 & 0.3 \\\\\n0.4 & 0.6\n\\end{bmatrix} Find its steady state vector.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nLet q = [x, y]^T be the steady state vector\nWrite Pq = q: \\begin{bmatrix} 0.7x + 0.3y = x \\\\ 0.4x + 0.6y = y \\end{bmatrix}\nUse x + y = 1\nSolve to get q = [0.571, 0.429]^T\n\n\n\n\n\n\n\nShow that if P is stochastic, then P^n is also stochastic for any positive integer n.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nUse induction on n\nBase case: True for n=1\nInductive step: Show if true for k, then true for k+1\nUse properties of matrix multiplication\n\n\n\n\n\n\n\nFor a regular stochastic matrix, prove that 1 is an eigenvalue and all other eigenvalues have absolute value less than 1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nShow that (1,v) is an eigenvalue-eigenvector pair where v is column of ones\nUse Gershgorin’s circle theorem to bound other eigenvalues\nUse regularity to show strict inequality"
  },
  {
    "objectID": "general_topics/predicting_the_future.html#additional-resources",
    "href": "general_topics/predicting_the_future.html#additional-resources",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "3Blue1Brown - Essence of Linear Algebra (YouTube)\nGilbert Strang’s Linear Algebra Lectures (MIT OpenCourseWare)\n“Introduction to Probability” by Bertsekas and Tsitsiklis\n“Matrix Analysis” by Horn and Johnson"
  },
  {
    "objectID": "general_topics/predicting_the_future.html#common-mistakes-to-avoid",
    "href": "general_topics/predicting_the_future.html#common-mistakes-to-avoid",
    "title": "Probability with Vectors/Matrices in Linear Algebra",
    "section": "",
    "text": "Forgetting to verify the stochastic property when computing matrix powers\nMisinterpreting steady state probabilities as transition probabilities\nAssuming all stochastic matrices have unique steady states (only true for regular matrices)\nNeglecting to normalize probability vectors after computations"
  },
  {
    "objectID": "general_topics/matrix_transformations.html",
    "href": "general_topics/matrix_transformations.html",
    "title": "Matrix Transformations",
    "section": "",
    "text": "Reference: Section 1.8\nDefinition: A transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is linear if it satisfies two properties:\n\nAdditivity: T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) for all vectors \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n.\nScalar Multiplication: T(c \\mathbf{u}) = c T(\\mathbf{u}) for any scalar c and vector \\mathbf{u} \\in \\mathbb{R}^n.\n\nExplanation: These properties mean that linear transformations preserve the structure of vector spaces. For instance, a scaling transformation will uniformly stretch all vectors by the same factor, maintaining the direction of the vectors.\nExample: If T(\\mathbf{x}) = A \\mathbf{x}, where A is a matrix, then T is a linear transformation because matrix multiplication inherently satisfies both additivity and scalar multiplication.\n\n\n\n\n\nReference: Sections 1.9 and 2.4\n“Onto” Transformation (Surjective): A transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is “onto” if every vector in \\mathbb{R}^m is the image of at least one vector in \\mathbb{R}^n. This implies that the range of T spans \\mathbb{R}^m.\n“One-to-One” Transformation (Injective): A transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is “one-to-one” if different vectors in \\mathbb{R}^n map to different vectors in \\mathbb{R}^m. This means that T(\\mathbf{u}) = T(\\mathbf{v}) implies \\mathbf{u} = \\mathbf{v}.\nExample and Explanation: If T(\\mathbf{x}) = A \\mathbf{x} and A is an m \\times n matrix:\n\nOnto: T is onto if the columns of A span \\mathbb{R}^m.\nOne-to-One: T is one-to-one if the columns of A are linearly independent, which holds if A has a pivot in every column.\n\n\n\n\n\n\nReference: Section 4.9\nDefinition: A Markov Chain is a sequence of states where the probability of moving to the next state depends only on the current state. This process can be represented with a transition matrix P, where each entry P_{ij} denotes the probability of transitioning from state i to state j.\nSteady State Solution: The steady state of a Markov Chain is a vector \\mathbf{s} such that P \\mathbf{s} = \\mathbf{s}. This vector represents a stable distribution of probabilities across states that no longer changes after transitions.\nExplanation: To find the steady state vector \\mathbf{s}, solve (P - I) \\mathbf{s} = 0, ensuring that the sum of the entries in \\mathbf{s} is 1 (since they represent probabilities).\nExample: For a transition matrix $ P =\n\\begin{bmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{bmatrix}\n, ] the steady state vector \\mathbf{s} = \\begin{bmatrix} s_1 \\\\ s_2 \\end{bmatrix} satisfies: $ P = . ] Solving this system, we can find the steady state probabilities for long-term behavior."
  },
  {
    "objectID": "general_topics/matrix_transformations.html#properties-for-a-transformation-to-be-a-linear-transformation",
    "href": "general_topics/matrix_transformations.html#properties-for-a-transformation-to-be-a-linear-transformation",
    "title": "Matrix Transformations",
    "section": "",
    "text": "Reference: Section 1.8\nDefinition: A transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is linear if it satisfies two properties:\n\nAdditivity: T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) for all vectors \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n.\nScalar Multiplication: T(c \\mathbf{u}) = c T(\\mathbf{u}) for any scalar c and vector \\mathbf{u} \\in \\mathbb{R}^n.\n\nExplanation: These properties mean that linear transformations preserve the structure of vector spaces. For instance, a scaling transformation will uniformly stretch all vectors by the same factor, maintaining the direction of the vectors.\nExample: If T(\\mathbf{x}) = A \\mathbf{x}, where A is a matrix, then T is a linear transformation because matrix multiplication inherently satisfies both additivity and scalar multiplication."
  },
  {
    "objectID": "general_topics/matrix_transformations.html#definitions-of-onto-and-one-to-one-transformations",
    "href": "general_topics/matrix_transformations.html#definitions-of-onto-and-one-to-one-transformations",
    "title": "Matrix Transformations",
    "section": "",
    "text": "Reference: Sections 1.9 and 2.4\n“Onto” Transformation (Surjective): A transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is “onto” if every vector in \\mathbb{R}^m is the image of at least one vector in \\mathbb{R}^n. This implies that the range of T spans \\mathbb{R}^m.\n“One-to-One” Transformation (Injective): A transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is “one-to-one” if different vectors in \\mathbb{R}^n map to different vectors in \\mathbb{R}^m. This means that T(\\mathbf{u}) = T(\\mathbf{v}) implies \\mathbf{u} = \\mathbf{v}.\nExample and Explanation: If T(\\mathbf{x}) = A \\mathbf{x} and A is an m \\times n matrix:\n\nOnto: T is onto if the columns of A span \\mathbb{R}^m.\nOne-to-One: T is one-to-one if the columns of A are linearly independent, which holds if A has a pivot in every column."
  },
  {
    "objectID": "general_topics/matrix_transformations.html#markov-chain-and-steady-state-solution",
    "href": "general_topics/matrix_transformations.html#markov-chain-and-steady-state-solution",
    "title": "Matrix Transformations",
    "section": "",
    "text": "Reference: Section 4.9\nDefinition: A Markov Chain is a sequence of states where the probability of moving to the next state depends only on the current state. This process can be represented with a transition matrix P, where each entry P_{ij} denotes the probability of transitioning from state i to state j.\nSteady State Solution: The steady state of a Markov Chain is a vector \\mathbf{s} such that P \\mathbf{s} = \\mathbf{s}. This vector represents a stable distribution of probabilities across states that no longer changes after transitions.\nExplanation: To find the steady state vector \\mathbf{s}, solve (P - I) \\mathbf{s} = 0, ensuring that the sum of the entries in \\mathbf{s} is 1 (since they represent probabilities).\nExample: For a transition matrix $ P =\n\\begin{bmatrix} 0.9 & 0.1 \\\\ 0.5 & 0.5 \\end{bmatrix}\n, ] the steady state vector \\mathbf{s} = \\begin{bmatrix} s_1 \\\\ s_2 \\end{bmatrix} satisfies: $ P = . ] Solving this system, we can find the steady state probabilities for long-term behavior."
  },
  {
    "objectID": "general_topics/linear_equations.html",
    "href": "general_topics/linear_equations.html",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "This guide outlines foundational topics in linear algebra related to systems of linear equations, row reduction methods, and matrix invertibility. It includes solutions to linear equations, elementary row operations, pivot positions, and the Invertible Matrix Theorem, equipping readers with core principles for understanding matrix theory.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A linear equation in variables x_1, \\dots, x_n has the form a_1x_1 + a_2x_2 + \\dots + a_nx_n = b, where b and each a_i are constants.\n\n\n\n\nThe equation x_2 = 2(\\sqrt{6} - x_1) + x_3 is linear, whereas 4x_1 - 5x_2 = x_1x_2 is nonlinear due to the term x_1x_2.\n\n\n\n\nA system of linear equations may have: 1. No solution (inconsistent). 2. A unique solution. 3. Infinitely many solutions.\nA system is consistent if it has at least one solution; otherwise, it is inconsistent.\n\n\n\nTo solve linear systems using matrices, we apply these operations: 1. Replacement: Replace one row by the sum of itself and a multiple of another row. 2. Interchange: Swap two rows. 3. Scaling: Multiply all entries in a row by a nonzero constant.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A matrix is in echelon form if: 1. All nonzero rows are above rows of all zeros. 2. Each leading entry is to the right of the leading entry in the row above it. 3. Entries below each leading entry are zeros.\n\n\nIf, in addition, each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced echelon form.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A pivot position is the location of a leading 1 in a matrix’s reduced echelon form. The corresponding column is a pivot column.\n\n\n\n\n\nBegin with the leftmost nonzero column (a pivot column) and move the pivot to the top row.\nCreate zeros below the pivot using row operations.\nCover the row containing the pivot and repeat with the submatrix formed by the rows below it.\nTo obtain the reduced echelon form, create zeros above each pivot by moving from the rightmost pivot up and left.\n\n\n\n\nTransform the matrix A = \\begin{bmatrix} 0 & -3 & -6 & 4 & 9 \\\\ -1 & -2 & -1 & 3 & 1 \\\\ -2 & -3 & 0 & 3 & -1 \\\\ 1 & 4 & 5 & -9 & -7 \\end{bmatrix} into echelon and then reduced echelon form. Identify pivot positions and columns.\n\n\n\n\n\n\n\n\n\n\n\nThe Uniqueness of the Reduced Echelon Form\n\n\n\nEvery matrix is row equivalent to a unique reduced echelon matrix, regardless of the row operations used.\n\n\n\n\n\n\n\nThe Invertible Matrix Theorem states that for an n \\times n matrix A, the following statements are equivalent:\n\nA is invertible.\nA is row equivalent to the identity matrix.\nA has n pivot positions.\nThe equation Ax = 0 has only the trivial solution.\nThe columns of A are linearly independent.\nThe transformation x \\to Ax is one-to-one.\nThe equation Ax = b has at least one solution for each b \\in \\mathbb{R}^n.\nThe columns of A span \\mathbb{R}^n.\nThe transformation x \\to Ax maps \\mathbb{R}^n onto \\mathbb{R}^n.\nThere exists a matrix C such that CA = I.\nThere exists a matrix D such that AD = I.\nA^T is invertible.\n\n\n\nIf matrices A and B satisfy AB = I, then both A and B are invertible with B = A^{-1} and A = B^{-1}.\n\n\n\n\n\n\n\nIn consistent systems with free variables, the solution set can be expressed parametrically, using free variables to define the set.\n\n\n\nTo test if a matrix is invertible, check for full pivot positions, or equivalently, attempt to reduce it to the identity matrix using row operations.\n\n\n\n\n\n\nIs the system x_2 - 4x_3 = 8 2x_1 - 3x_2 + 2x_3 = 1 5x_1 - 8x_2 + 7x_3 = 1 consistent?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFormulate the augmented matrix and row reduce to echelon form. Check if there is a row that implies inconsistency.\n\n\n\n\n\n\nVerify the invertibility of the matrix \\begin{bmatrix} -4 & 6 \\\\ 6 & -9 \\end{bmatrix}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCalculate the determinant. A nonzero result confirms invertibility. Alternatively, apply row reduction to check if it’s row equivalent to the identity matrix.\n\n\n\n\n\n\n\n\nLinear Algebra and Its Applications by Gilbert Strang\nMIT OpenCourseWare - Linear Algebra\nKhan Academy - Linear Algebra"
  },
  {
    "objectID": "general_topics/linear_equations.html#introduction",
    "href": "general_topics/linear_equations.html#introduction",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "This guide outlines foundational topics in linear algebra related to systems of linear equations, row reduction methods, and matrix invertibility. It includes solutions to linear equations, elementary row operations, pivot positions, and the Invertible Matrix Theorem, equipping readers with core principles for understanding matrix theory."
  },
  {
    "objectID": "general_topics/linear_equations.html#key-concepts-and-definitions",
    "href": "general_topics/linear_equations.html#key-concepts-and-definitions",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "Note\n\n\n\nDefinition: A linear equation in variables x_1, \\dots, x_n has the form a_1x_1 + a_2x_2 + \\dots + a_nx_n = b, where b and each a_i are constants.\n\n\n\n\nThe equation x_2 = 2(\\sqrt{6} - x_1) + x_3 is linear, whereas 4x_1 - 5x_2 = x_1x_2 is nonlinear due to the term x_1x_2.\n\n\n\n\nA system of linear equations may have: 1. No solution (inconsistent). 2. A unique solution. 3. Infinitely many solutions.\nA system is consistent if it has at least one solution; otherwise, it is inconsistent.\n\n\n\nTo solve linear systems using matrices, we apply these operations: 1. Replacement: Replace one row by the sum of itself and a multiple of another row. 2. Interchange: Swap two rows. 3. Scaling: Multiply all entries in a row by a nonzero constant."
  },
  {
    "objectID": "general_topics/linear_equations.html#row-reduction-and-echelon-forms-section-1.2",
    "href": "general_topics/linear_equations.html#row-reduction-and-echelon-forms-section-1.2",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "Note\n\n\n\nDefinition: A matrix is in echelon form if: 1. All nonzero rows are above rows of all zeros. 2. Each leading entry is to the right of the leading entry in the row above it. 3. Entries below each leading entry are zeros.\n\n\nIf, in addition, each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced echelon form.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A pivot position is the location of a leading 1 in a matrix’s reduced echelon form. The corresponding column is a pivot column.\n\n\n\n\n\nBegin with the leftmost nonzero column (a pivot column) and move the pivot to the top row.\nCreate zeros below the pivot using row operations.\nCover the row containing the pivot and repeat with the submatrix formed by the rows below it.\nTo obtain the reduced echelon form, create zeros above each pivot by moving from the rightmost pivot up and left.\n\n\n\n\nTransform the matrix A = \\begin{bmatrix} 0 & -3 & -6 & 4 & 9 \\\\ -1 & -2 & -1 & 3 & 1 \\\\ -2 & -3 & 0 & 3 & -1 \\\\ 1 & 4 & 5 & -9 & -7 \\end{bmatrix} into echelon and then reduced echelon form. Identify pivot positions and columns."
  },
  {
    "objectID": "general_topics/linear_equations.html#important-theorems",
    "href": "general_topics/linear_equations.html#important-theorems",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "The Uniqueness of the Reduced Echelon Form\n\n\n\nEvery matrix is row equivalent to a unique reduced echelon matrix, regardless of the row operations used."
  },
  {
    "objectID": "general_topics/linear_equations.html#invertible-matrices-and-the-invertible-matrix-theorem-section-2.3",
    "href": "general_topics/linear_equations.html#invertible-matrices-and-the-invertible-matrix-theorem-section-2.3",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "The Invertible Matrix Theorem states that for an n \\times n matrix A, the following statements are equivalent:\n\nA is invertible.\nA is row equivalent to the identity matrix.\nA has n pivot positions.\nThe equation Ax = 0 has only the trivial solution.\nThe columns of A are linearly independent.\nThe transformation x \\to Ax is one-to-one.\nThe equation Ax = b has at least one solution for each b \\in \\mathbb{R}^n.\nThe columns of A span \\mathbb{R}^n.\nThe transformation x \\to Ax maps \\mathbb{R}^n onto \\mathbb{R}^n.\nThere exists a matrix C such that CA = I.\nThere exists a matrix D such that AD = I.\nA^T is invertible.\n\n\n\nIf matrices A and B satisfy AB = I, then both A and B are invertible with B = A^{-1} and A = B^{-1}."
  },
  {
    "objectID": "general_topics/linear_equations.html#special-cases-or-applications",
    "href": "general_topics/linear_equations.html#special-cases-or-applications",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "In consistent systems with free variables, the solution set can be expressed parametrically, using free variables to define the set.\n\n\n\nTo test if a matrix is invertible, check for full pivot positions, or equivalently, attempt to reduce it to the identity matrix using row operations."
  },
  {
    "objectID": "general_topics/linear_equations.html#practice-problems",
    "href": "general_topics/linear_equations.html#practice-problems",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "Is the system x_2 - 4x_3 = 8 2x_1 - 3x_2 + 2x_3 = 1 5x_1 - 8x_2 + 7x_3 = 1 consistent?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFormulate the augmented matrix and row reduce to echelon form. Check if there is a row that implies inconsistency.\n\n\n\n\n\n\nVerify the invertibility of the matrix \\begin{bmatrix} -4 & 6 \\\\ 6 & -9 \\end{bmatrix}.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCalculate the determinant. A nonzero result confirms invertibility. Alternatively, apply row reduction to check if it’s row equivalent to the identity matrix."
  },
  {
    "objectID": "general_topics/linear_equations.html#additional-resources",
    "href": "general_topics/linear_equations.html#additional-resources",
    "title": "Linear Algebra Essentials: Systems, Row Reduction, and Invertibility",
    "section": "",
    "text": "Linear Algebra and Its Applications by Gilbert Strang\nMIT OpenCourseWare - Linear Algebra\nKhan Academy - Linear Algebra"
  },
  {
    "objectID": "general_topics/determinants.html",
    "href": "general_topics/determinants.html",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "This study guide covers essential concepts about determinants in linear algebra, focusing on the effects of row operations on determinants and the relationship between a matrix’s invertibility and its determinant.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The determinant of a square matrix A is a scalar that provides information about the matrix’s properties, such as invertibility and volume scaling factor. For a 2x2 matrix, the determinant is calculated as: \\text{det}(A) = ad - bc \\quad \\text{for} \\quad A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\n\n\n\nFor A = \\begin{bmatrix} 3 & 4 \\\\ 2 & 5 \\end{bmatrix}, the determinant is: \\text{det}(A) = (3 \\times 5) - (4 \\times 2) = 15 - 8 = 7.\n\n\n\n\nThe determinant of a matrix A is: - Zero if A is not invertible. - Nonzero if A is invertible.\nThe determinant also changes in predictable ways with row operations, as detailed below.\n\n\n\n\n\n\n\nRow Replacement: Replacing one row with itself plus a multiple of another row leaves the determinant unchanged.\nRow Interchange: Interchanging two rows of a matrix multiplies the determinant by (-1).\nRow Scaling: Multiplying a row by a scalar k scales the determinant by k.\n\n\n\nConsider A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} with \\text{det}(A) = -2. - Row Replacement: Replacing row 1 with \\text{row 1} + 3 \\times \\text{row 2} does not alter the determinant. - Row Interchange: Swapping rows 1 and 2 changes the determinant to 2. - Row Scaling: Multiplying row 1 by 3 changes the determinant to -6.\n\n\n\n\nDeterminants can be computed by transforming the matrix to an upper triangular form via row operations. The determinant is then the product of the diagonal entries.\n\n\n\n\n\n\n\n\n\n\nDeterminant Multiplication Theorem\n\n\n\nFor matrices A and B of the same size, \\text{det}(AB) = \\text{det}(A) \\times \\text{det}(B).\n\n\n\n\n\n\n\n\nInvertibility and Determinants\n\n\n\nA matrix A is invertible if and only if \\text{det}(A) \\neq 0.\n\n\n\n\n\n\n\nIf \\text{det}(A) = 0, then: - A is not invertible. - A has linearly dependent rows or columns. - The matrix A maps volumes to zero, indicating a lack of full rank.\nConversely, if \\text{det}(A) \\neq 0, then A is invertible, has linearly independent columns, and the transformation represented by A is volume-preserving (up to a scaling factor).\n\n\nFor A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}, \\text{det}(A) = (1)(6) - (2)(3) = 0. Thus, A is not invertible due to linearly dependent columns.\n\n\n\n\n\n\n\nThe determinant of a transformation matrix represents the scaling factor for areas (in 2D) or volumes (in 3D) under the transformation. A determinant of zero implies that the transformation collapses the space into a lower dimension.\n\n\n\nFor a triangular matrix, the determinant is the product of its diagonal entries. This property simplifies determinant calculations, especially after transforming a matrix to echelon form.\n\n\n\n\n\n\nCalculate the determinant of the matrix\nA = \\begin{bmatrix} 2 & 5 \\\\ 1 & 3 \\end{bmatrix}\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the formula \\text{det}(A) = ad - bc:\n\\text{det}(A) = (2 \\times 3) - (5 \\times 1) = 6 - 5 = 1\nSince \\text{det}(A) \\neq 0, A is invertible.\n\n\n\n\n\n\nDetermine if the matrix B = \\begin{bmatrix} 0 & 1 \\\\ 4 & 0 \\end{bmatrix} is invertible using its determinant.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\text{det}(B) = (0 \\times 0) - (1 \\times 4) = -4. Since \\text{det}(B) \\neq 0, B is invertible.\n\n\n\n\n\n\n\n\nLinear Algebra Done Right by Sheldon Axler\nIntroduction to Linear Algebra by Gilbert Strang\n3Blue1Brown - Essence of Linear Algebra (YouTube)"
  },
  {
    "objectID": "general_topics/determinants.html#introduction",
    "href": "general_topics/determinants.html#introduction",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "This study guide covers essential concepts about determinants in linear algebra, focusing on the effects of row operations on determinants and the relationship between a matrix’s invertibility and its determinant."
  },
  {
    "objectID": "general_topics/determinants.html#key-concepts-and-definitions",
    "href": "general_topics/determinants.html#key-concepts-and-definitions",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "Note\n\n\n\nDefinition: The determinant of a square matrix A is a scalar that provides information about the matrix’s properties, such as invertibility and volume scaling factor. For a 2x2 matrix, the determinant is calculated as: \\text{det}(A) = ad - bc \\quad \\text{for} \\quad A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\n\n\n\nFor A = \\begin{bmatrix} 3 & 4 \\\\ 2 & 5 \\end{bmatrix}, the determinant is: \\text{det}(A) = (3 \\times 5) - (4 \\times 2) = 15 - 8 = 7.\n\n\n\n\nThe determinant of a matrix A is: - Zero if A is not invertible. - Nonzero if A is invertible.\nThe determinant also changes in predictable ways with row operations, as detailed below."
  },
  {
    "objectID": "general_topics/determinants.html#effects-of-row-operations-on-determinants",
    "href": "general_topics/determinants.html#effects-of-row-operations-on-determinants",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "Row Replacement: Replacing one row with itself plus a multiple of another row leaves the determinant unchanged.\nRow Interchange: Interchanging two rows of a matrix multiplies the determinant by (-1).\nRow Scaling: Multiplying a row by a scalar k scales the determinant by k.\n\n\n\nConsider A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} with \\text{det}(A) = -2. - Row Replacement: Replacing row 1 with \\text{row 1} + 3 \\times \\text{row 2} does not alter the determinant. - Row Interchange: Swapping rows 1 and 2 changes the determinant to 2. - Row Scaling: Multiplying row 1 by 3 changes the determinant to -6.\n\n\n\n\nDeterminants can be computed by transforming the matrix to an upper triangular form via row operations. The determinant is then the product of the diagonal entries."
  },
  {
    "objectID": "general_topics/determinants.html#important-theorems",
    "href": "general_topics/determinants.html#important-theorems",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "Determinant Multiplication Theorem\n\n\n\nFor matrices A and B of the same size, \\text{det}(AB) = \\text{det}(A) \\times \\text{det}(B).\n\n\n\n\n\n\n\n\nInvertibility and Determinants\n\n\n\nA matrix A is invertible if and only if \\text{det}(A) \\neq 0."
  },
  {
    "objectID": "general_topics/determinants.html#relationship-between-invertibility-and-determinants",
    "href": "general_topics/determinants.html#relationship-between-invertibility-and-determinants",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "If \\text{det}(A) = 0, then: - A is not invertible. - A has linearly dependent rows or columns. - The matrix A maps volumes to zero, indicating a lack of full rank.\nConversely, if \\text{det}(A) \\neq 0, then A is invertible, has linearly independent columns, and the transformation represented by A is volume-preserving (up to a scaling factor).\n\n\nFor A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}, \\text{det}(A) = (1)(6) - (2)(3) = 0. Thus, A is not invertible due to linearly dependent columns."
  },
  {
    "objectID": "general_topics/determinants.html#special-cases-or-applications",
    "href": "general_topics/determinants.html#special-cases-or-applications",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "The determinant of a transformation matrix represents the scaling factor for areas (in 2D) or volumes (in 3D) under the transformation. A determinant of zero implies that the transformation collapses the space into a lower dimension.\n\n\n\nFor a triangular matrix, the determinant is the product of its diagonal entries. This property simplifies determinant calculations, especially after transforming a matrix to echelon form."
  },
  {
    "objectID": "general_topics/determinants.html#practice-problems",
    "href": "general_topics/determinants.html#practice-problems",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "Calculate the determinant of the matrix\nA = \\begin{bmatrix} 2 & 5 \\\\ 1 & 3 \\end{bmatrix}\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the formula \\text{det}(A) = ad - bc:\n\\text{det}(A) = (2 \\times 3) - (5 \\times 1) = 6 - 5 = 1\nSince \\text{det}(A) \\neq 0, A is invertible.\n\n\n\n\n\n\nDetermine if the matrix B = \\begin{bmatrix} 0 & 1 \\\\ 4 & 0 \\end{bmatrix} is invertible using its determinant.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\text{det}(B) = (0 \\times 0) - (1 \\times 4) = -4. Since \\text{det}(B) \\neq 0, B is invertible."
  },
  {
    "objectID": "general_topics/determinants.html#additional-resources",
    "href": "general_topics/determinants.html#additional-resources",
    "title": "Determinants and Invertibility in Linear Algebra",
    "section": "",
    "text": "Linear Algebra Done Right by Sheldon Axler\nIntroduction to Linear Algebra by Gilbert Strang\n3Blue1Brown - Essence of Linear Algebra (YouTube)"
  },
  {
    "objectID": "exam4/chapter5_3.html",
    "href": "exam4/chapter5_3.html",
    "title": "Chapter 5.3",
    "section": "",
    "text": "In linear algebra, diagonality refers to the process of diagonalizing a matrix, which is a similarity transformation that produces a diagonal matrix. A diagonal matrix is a square matrix where all elements are zero except for those on the main diagonal.\n\n1 Matrix\n\nA = P D P^{-1}\n\\\\\n\\text{For } k \\geq 1 \\text{,}\n\\\\\nA^k = P D^{k} P^{-1}\n\n\n\n\n\n\n\nTheorem 5\n\n\n\n\n\nAn n ~ \\text{x} ~ n matrix A is diagonalizable if and only if A has n linearly independent eigenvectors.\nAKA\nA is diaganolizable if and only if there are enough eigenvectors to form a basis of \\real^n (thus forming the eigenvecotr basis of \\real^n)\n\n\n\n\n\n\n\n\n\nTheorem 7\n\n\n\n\n\nLet A be an n \\times n matrix with distinct eigenvalues \\lambda_1, \\dots, \\lambda_p:\n\nDimension of eigenspaces: For each eigenvalue \\lambda_k, the dimension of its eigenspace is less than or equal to its algebraic multiplicity.\nDiagonalizability: A is diagonalizable if and only if:\n\nThe sum of the dimensions of all eigenspaces equals n, and\nThe characteristic polynomial of A splits into distinct linear factors (no repeated roots in irreducible form).\n\nEigenvector basis: If A is diagonalizable, the union of bases of all eigenspaces forms an eigenvector basis for \\mathbb{R}^n.\n\n\n\n\n\n\n2 Diagonalization theorem (5) steps\nHow to find PDP^-1:\n\nFind eigenvalues of A.\n\ndet(A-\\lambda I) = 0\n\nif &lt; n are found, then A cannot be diagonalized.\n\n\nFind n linearly independent eigenvectors of A.\n\n\\vec{v_n} = (A-\\lambda I)x = 0 where \\lambda are the ones found in Step 1\n\nConstruct P from the vectors found in Step 2\n\nP= [\\vec{v_1}, ... , \\vec{v_n}]\n\nConstruct D from eigenvalues found in Step 1\n\nD = \\begin{bmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_n \\end{bmatrix}\n\nValidate A, D, and P\n\nAP = PD or A = PDP^{-1}"
  },
  {
    "objectID": "exam3/every_three_days_review.html",
    "href": "exam3/every_three_days_review.html",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "",
    "text": "Note\n\n\n\nDefinition: An inner product space is a vector space equipped with an inner product \\langle \\mathbf{u}, \\mathbf{v} \\rangle that satisfies: 1. Symmetry: \\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle} 2. Linearity: \\langle a\\mathbf{u} + \\mathbf{w}, \\mathbf{v} \\rangle = a\\langle \\mathbf{u}, \\mathbf{v} \\rangle + \\langle \\mathbf{w}, \\mathbf{v} \\rangle 3. Positive definiteness: \\langle \\mathbf{u}, \\mathbf{u} \\rangle &gt; 0 for \\mathbf{u} \\neq \\mathbf{0}\n\n\n\n\n\nNorm: \\|\\mathbf{v}\\| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\nOrthogonality: \\mathbf{u} \\perp \\mathbf{v} if \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\nCauchy-Schwarz: |\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\n\n\n\n\nFor vectors in \\mathbb{R}^n, find the angle between \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nUse the formula: \\cos \\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\nCalculate inner product: \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 1(2) + 2(1) = 4\nCalculate norms:\n\n\\|\\mathbf{u}\\| = \\sqrt{1^2 + 2^2} = \\sqrt{5}\n\\|\\mathbf{v}\\| = \\sqrt{2^2 + 1^2} = \\sqrt{5}\n\nTherefore: \\cos \\theta = \\frac{4}{\\sqrt{5}\\sqrt{5}} = \\frac{4}{5}\n\\theta = \\arccos(\\frac{4}{5}) \\approx 36.9°\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The orthogonal complement W^\\perp of a subspace W consists of all vectors orthogonal to every vector in W.\n\n\n\n\n\nIf W is a subspace of \\mathbb{R}^n, then:\n\n\\dim(W) + \\dim(W^\\perp) = n\n(W^\\perp)^\\perp = W\nW \\cap W^\\perp = \\{\\mathbf{0}\\}\n\n\n\n\n\nFind the orthogonal complement of W = \\text{span}\\{\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\} in \\mathbb{R}^3.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nLet \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} be in W^\\perp\nThen \\langle \\mathbf{x}, \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\rangle = 0\nThis gives: x + y = 0\nTherefore: W^\\perp = \\text{span}\\{\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The orthogonal projection of \\mathbf{v} onto \\mathbf{u} is: \\text{proj}_\\mathbf{u}(\\mathbf{v}) = \\frac{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u}\n\n\n\n\n\n\\text{proj}_\\mathbf{u}(\\mathbf{v}) is parallel to \\mathbf{u}\n\\mathbf{v} - \\text{proj}_\\mathbf{u}(\\mathbf{v}) is orthogonal to \\mathbf{u}\n\\|\\text{proj}_\\mathbf{u}(\\mathbf{v})\\| \\leq \\|\\mathbf{v}\\|"
  },
  {
    "objectID": "exam3/every_three_days_review.html#inner-product-spaces",
    "href": "exam3/every_three_days_review.html#inner-product-spaces",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "",
    "text": "Note\n\n\n\nDefinition: An inner product space is a vector space equipped with an inner product \\langle \\mathbf{u}, \\mathbf{v} \\rangle that satisfies: 1. Symmetry: \\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle} 2. Linearity: \\langle a\\mathbf{u} + \\mathbf{w}, \\mathbf{v} \\rangle = a\\langle \\mathbf{u}, \\mathbf{v} \\rangle + \\langle \\mathbf{w}, \\mathbf{v} \\rangle 3. Positive definiteness: \\langle \\mathbf{u}, \\mathbf{u} \\rangle &gt; 0 for \\mathbf{u} \\neq \\mathbf{0}\n\n\n\n\n\nNorm: \\|\\mathbf{v}\\| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\nOrthogonality: \\mathbf{u} \\perp \\mathbf{v} if \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\nCauchy-Schwarz: |\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\n\n\n\n\nFor vectors in \\mathbb{R}^n, find the angle between \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nUse the formula: \\cos \\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\nCalculate inner product: \\langle \\mathbf{u}, \\mathbf{v} \\rangle = 1(2) + 2(1) = 4\nCalculate norms:\n\n\\|\\mathbf{u}\\| = \\sqrt{1^2 + 2^2} = \\sqrt{5}\n\\|\\mathbf{v}\\| = \\sqrt{2^2 + 1^2} = \\sqrt{5}\n\nTherefore: \\cos \\theta = \\frac{4}{\\sqrt{5}\\sqrt{5}} = \\frac{4}{5}\n\\theta = \\arccos(\\frac{4}{5}) \\approx 36.9°"
  },
  {
    "objectID": "exam3/every_three_days_review.html#orthogonal-complement",
    "href": "exam3/every_three_days_review.html#orthogonal-complement",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "",
    "text": "Note\n\n\n\nDefinition: The orthogonal complement W^\\perp of a subspace W consists of all vectors orthogonal to every vector in W.\n\n\n\n\n\nIf W is a subspace of \\mathbb{R}^n, then:\n\n\\dim(W) + \\dim(W^\\perp) = n\n(W^\\perp)^\\perp = W\nW \\cap W^\\perp = \\{\\mathbf{0}\\}\n\n\n\n\n\nFind the orthogonal complement of W = \\text{span}\\{\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\} in \\mathbb{R}^3.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nLet \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} be in W^\\perp\nThen \\langle \\mathbf{x}, \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\rangle = 0\nThis gives: x + y = 0\nTherefore: W^\\perp = \\text{span}\\{\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\}"
  },
  {
    "objectID": "exam3/every_three_days_review.html#projection",
    "href": "exam3/every_three_days_review.html#projection",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "",
    "text": "Note\n\n\n\nDefinition: The orthogonal projection of \\mathbf{v} onto \\mathbf{u} is: \\text{proj}_\\mathbf{u}(\\mathbf{v}) = \\frac{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u}\n\n\n\n\n\n\\text{proj}_\\mathbf{u}(\\mathbf{v}) is parallel to \\mathbf{u}\n\\mathbf{v} - \\text{proj}_\\mathbf{u}(\\mathbf{v}) is orthogonal to \\mathbf{u}\n\\|\\text{proj}_\\mathbf{u}(\\mathbf{v})\\| \\leq \\|\\mathbf{v}\\|"
  },
  {
    "objectID": "exam3/every_three_days_review.html#lu-decomposition",
    "href": "exam3/every_three_days_review.html#lu-decomposition",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "2.1 LU Decomposition",
    "text": "2.1 LU Decomposition\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For a square matrix A, the LU decomposition is A = LU where: - L is lower triangular with 1’s on diagonal - U is upper triangular\n\n\n\n2.1.1 Example 3: LU Decomposition\nFind the LU decomposition of A = \\begin{bmatrix} 2 & 1 \\\\ 4 & 3 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nFirst row remains unchanged for U\nMultiply first row by 2 and subtract from second row\nResult: L = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}, U = \\begin{bmatrix} 2 & 1 \\\\ 0 & 1 \\end{bmatrix}\nVerify: LU = A"
  },
  {
    "objectID": "exam3/every_three_days_review.html#qr-decomposition",
    "href": "exam3/every_three_days_review.html#qr-decomposition",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "2.2 QR Decomposition",
    "text": "2.2 QR Decomposition\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For a matrix A, the QR decomposition is A = QR where: - Q is orthogonal (Q^TQ = I) - R is upper triangular\n\n\n\n2.2.1 Applications\n\nSolving linear systems\nLeast squares problems\nComputing eigenvalues (QR algorithm)"
  },
  {
    "objectID": "exam3/every_three_days_review.html#singular-value-decomposition-svd",
    "href": "exam3/every_three_days_review.html#singular-value-decomposition-svd",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "2.3 Singular Value Decomposition (SVD)",
    "text": "2.3 Singular Value Decomposition (SVD)\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For any matrix A, the SVD is A = U\\Sigma V^T where: - U and V are orthogonal - \\Sigma is diagonal with non-negative entries (singular values)\n\n\n\n2.3.1 Properties\n\nSingular values \\sigma_i are unique\nNumber of non-zero singular values = rank of A\n\\|A\\|_2 = \\sigma_1 (largest singular value)"
  },
  {
    "objectID": "exam3/every_three_days_review.html#symmetric-matrices",
    "href": "exam3/every_three_days_review.html#symmetric-matrices",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "3.1 Symmetric Matrices",
    "text": "3.1 Symmetric Matrices\n\n3.1.1 Properties\n\nAll eigenvalues are real\nEigenvectors of distinct eigenvalues are orthogonal\nDiagonalizable with orthogonal eigenvector matrix\n\n\n\n3.1.2 Example 4: Symmetric Matrix Properties\nVerify the properties for A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nCharacteristic equation: (3-\\lambda)^2 - 1 = 0\nEigenvalues: \\lambda = 4, 2 (real)\nEigenvectors: \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} (orthogonal)"
  },
  {
    "objectID": "exam3/every_three_days_review.html#positive-definite-matrices",
    "href": "exam3/every_three_days_review.html#positive-definite-matrices",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "3.2 Positive Definite Matrices",
    "text": "3.2 Positive Definite Matrices\n\n3.2.1 Equivalent Conditions\n\nAll eigenvalues are positive\nAll leading principal minors are positive\n\\mathbf{x}^T A \\mathbf{x} &gt; 0 for all non-zero \\mathbf{x}\nExistence of unique Cholesky decomposition A = LL^T"
  },
  {
    "objectID": "exam3/every_three_days_review.html#systems-of-linear-odes",
    "href": "exam3/every_three_days_review.html#systems-of-linear-odes",
    "title": "Supporting Topics (Review Twice Weekly)",
    "section": "4.1 Systems of Linear ODEs",
    "text": "4.1 Systems of Linear ODEs\n\n4.1.1 General Form\n\\frac{d\\mathbf{x}}{dt} = A\\mathbf{x}\n\n\n4.1.2 Solution Method\n\nFind eigenvalues and eigenvectors of A\nSolution: \\mathbf{x}(t) = c_1e^{\\lambda_1t}\\mathbf{v}_1 + c_2e^{\\lambda_2t}\\mathbf{v}_2\n\n\n\n4.1.3 Example 5: Solving System of ODEs\nSolve \\begin{bmatrix} x'(t) \\\\ y'(t) \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}\\begin{bmatrix} x(t) \\\\ y(t) \\end{bmatrix}\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nEigenvalues: \\lambda = 3, -1\nEigenvectors: \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\nGeneral solution: \\mathbf{x}(t) = c_1e^{3t}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + c_2e^{-t}\\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}"
  },
  {
    "objectID": "class.html",
    "href": "class.html",
    "title": "Class Schedule",
    "section": "",
    "text": "Class Date\nPreparation\nClass\nHomework\nHomework Due Dates\n\n\n\n\n9/16\nIntro and 1.1\n\n1, 2, 5, 7, 9, 13, 17, 19, 23, 24, 31; Self Testing 1.1\n\n\n\n9/18\nDo Self Testing 1.1; read 1.2\n1.2\n1, 3, 4, 5, 7, 8, 12, 14, 16, 17, 24, 29; S.T. 1.2\n1.1 due\n\n\n9/20\nDo Self Testing 1.2; read 1.3\n1.3\n3, 5, 7, 9, 12, 13, 15, 19, 24, 25; S.T. 1.3\n1.2 due\n\n\n9/23\nEtc. as above\n1.4\n1, 3, 5, 7, 10, 12, 13, 15, 22, 25, 26, 32, 40 (use Mathematica for 40); S.T. 1.4\n\n\n\n9/25\n\n1.5\n3, 5, 7, 11, 15, 20, 23, 25, 29, 30, 35; S.T. 1.5\n1.3, 1.4 due\n\n\n9/27\n\n1.7\n1, 3, 4, 7, 16, 18, 20, 21, 26, 33, 36; S.T. 1.7\n1.5 due\n\n\n9/30\nExam Review\n\nExam 1 (Monday-Wednesday)\n\n\n\n10/2\n\n1.6\nNone\n1.7 due\n\n\n10/4\n\n1.8\n1, 5, 8, 9, 13, 14, 17, 21, 29, 31, 39; S.T. 1.8\n\n\n\n10/7\n\n1.9\n3, 8, 16, 19, 27, 29, 31, 32, 35; S.T. 1.9\n1.8 due\n\n\n10/9\n\n2.1\n1, 3, 6, 7, 10, 11, 12, 16, 22 (hint: use the definition of linear dependence), 27\n\n\n\n10/11\n\n2.2\n1, 3, 6, 8, 9, 13, 14, 18, 23, 29, 32; S.T. 2.2\n1.9 due\n\n\n10/14\n\n2.3\n2, 4, 5, 11, 13, 15, 16, 17, 28, 31, 33; S.T. 2.3\n2.1, 2.2 due\n\n\n10/16\nPython and 2.4\n2.4\n1, 7, 10, 13, 17, 21; Programming hwk 1\n\n\n\n10/18\n\n3.1\n1, 2, 9, 12, 15, 19, 20, 21, 37\n2.3 due\n\n\n10/21\n\n3.2\n1, 2, 3, 5, 9, 15, 18, 21, 24, 31, 40, 41; S.T. 3.2\n2.4, Programming hwk 1, and 3.1 due\n\n\n10/23\nExam Review\n\n3.2 due; Exam 2 (Wed-Fri)\n\n\n\n10/25\nTest Day\n\nExam 2\n\n\n\n10/28\n\n4.1\n1, 3, 6, 7, 8, 9, 11, 13, 15, 17, 21, 26; S.T. 4.1\n\n\n\n10/30\n\n4.1\n\n\n\n\n11/1\n\n4.2\n2, 3, 5, 7, 8, 18, 21, 23, 26 (no justification needed), 33; S.T. 4.2\n4.1 due\n\n\n11/4\n\n4.3\n1, 2, 8, 13, 14, 19, 22 (a, b, c), 29, 30, 33; S.T. 4.3\n4.2 due\n\n\n11/6\n\n4.4\n1, 3, 6, 7, 9, 11, 27, 32\n\n\n\n11/8\n\n4.5\n1, 10, 11, 13, 14, 17, 21, 28 (hint: consider a subspace that we have looked at several times),\n4.3 due\n\n\n\n\n\n29, 30 (use a computer/calculator if needed); S.T. 4.5\n\n\n\n11/11\n\n4.6\n1, 2, 5, 6, 7, 8, 15, 19, 26, 31; S.T. 4.6\n4.4, 4.5 due\n\n\n11/13\n\n4.9\n2, 3, 5, 9, 12, 13; S.T. 4.9\n\n\n\n11/15\n\n5.1\n3, 6, 9, 14, 17, 19, 23, 25, 26, 27, 29; S.T. 5.1; Programming 2\n4.6, 4.9 due\n\n\n11/18\n\n5.2\n1, 3, 7, 11, 15, 16, 18, 19, 20, 24; S.T. 5.2\n5.1 due\n\n\n11/20\n\n5.3\n1, 6, 7, 9, 12, 21, 25, 27, 31\n\n\n\n11/22\nReview\n\nExam Review; Programming hwk 2 due; 5.3 due (Exam Fri-Tues)\n\n\n\n11/25\nExam Day\n\nExam 3 (covers 4.1-4.9)\n\n\n\n11/27-29\nHoliday\n\n\n\n\n\n12/2\n\n6.1\n1, 5, 7, 9, 11, 14, 15, 16, 19, 28, 30; S.T. 6.1\n\n\n\n12/4\n\n6.2\n1, 5, 7, 11, 14, 17, 23 (a, b, c), 27; S.T. 6.2\n6.1 due\n\n\n12/6\n\n6.3\n1, 5, 8, 12, 16, 19, 21 (a, b, c, d)\n6.2 due\n\n\n12/9\n\n6.4\n3, 5, 7, 9, 17, 19\n\n\n\n12/11\n\n6.5\n1, 3, 5, 7, 13, 17\n6.3 due\n\n\n12/13\n\n6.6\n1, 3, 5, 8 (recommend using Python for calculations); S.T. 6.6\n6.4, 6.5 due\n\n\n12/16\nExam Review\n\n\n6.6 due\n\n\n12/17-18\nUniversity Testing Days\n\nExam 4 (Mon-Wed); Final is ~50% new (chpt 5, 6) and 50% old"
  },
  {
    "objectID": "exam3/daily_review.html",
    "href": "exam3/daily_review.html",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "A vector space is a set V with two operations: addition and scalar multiplication, satisfying axioms like associativity, distributivity, and the existence of zero and additive inverses.\n\n\n\n\n\nThree Key Properties:\n\nClosed under addition (\\vec{u} + \\vec{v} \\in H).\nClosed under scalar multiplication (c\\vec{u} \\in H).\nContains the zero vector (\\vec{0} \\in H).\n\nCommon Examples: Null space, column space, solution spaces to homogeneous equations.\n\n\n\n\n\nDefinition: A set of vectors is linearly independent if no vector in the set is a linear combination of the others.\nCommon Mistake: Confusing dependence with spanning.\n\n\n\n\n\nA set spans V if every element in V can be written as a linear combination of the set.\n\n\n\n\n\nA basis is a linearly independent spanning set.\n\n\n\n\n\nDefined as the number of vectors in any basis for V."
  },
  {
    "objectID": "exam3/daily_review.html#vector-space-definition-properties",
    "href": "exam3/daily_review.html#vector-space-definition-properties",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "A vector space is a set V with two operations: addition and scalar multiplication, satisfying axioms like associativity, distributivity, and the existence of zero and additive inverses."
  },
  {
    "objectID": "exam3/daily_review.html#subspaces",
    "href": "exam3/daily_review.html#subspaces",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "Three Key Properties:\n\nClosed under addition (\\vec{u} + \\vec{v} \\in H).\nClosed under scalar multiplication (c\\vec{u} \\in H).\nContains the zero vector (\\vec{0} \\in H).\n\nCommon Examples: Null space, column space, solution spaces to homogeneous equations."
  },
  {
    "objectID": "exam3/daily_review.html#linear-independence-dependence",
    "href": "exam3/daily_review.html#linear-independence-dependence",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "Definition: A set of vectors is linearly independent if no vector in the set is a linear combination of the others.\nCommon Mistake: Confusing dependence with spanning."
  },
  {
    "objectID": "exam3/daily_review.html#spanning-sets",
    "href": "exam3/daily_review.html#spanning-sets",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "A set spans V if every element in V can be written as a linear combination of the set."
  },
  {
    "objectID": "exam3/daily_review.html#basis",
    "href": "exam3/daily_review.html#basis",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "A basis is a linearly independent spanning set."
  },
  {
    "objectID": "exam3/daily_review.html#dimension",
    "href": "exam3/daily_review.html#dimension",
    "title": "Critical Foundations (Review Daily)",
    "section": "",
    "text": "Defined as the number of vectors in any basis for V."
  },
  {
    "objectID": "exam3/daily_review.html#null-space",
    "href": "exam3/daily_review.html#null-space",
    "title": "Critical Foundations (Review Daily)",
    "section": "2.1 Null Space",
    "text": "2.1 Null Space\n\nDefinition: Set of all solutions to A\\mathbf{x} = 0.\nComputation: Solve A\\mathbf{x} = 0 via row reduction."
  },
  {
    "objectID": "exam3/daily_review.html#column-space",
    "href": "exam3/daily_review.html#column-space",
    "title": "Critical Foundations (Review Daily)",
    "section": "2.2 Column Space",
    "text": "2.2 Column Space\n\nDefinition: Span of the column vectors of a matrix.\nFinding Basis: Use pivot columns in RREF."
  },
  {
    "objectID": "exam3/daily_review.html#row-space",
    "href": "exam3/daily_review.html#row-space",
    "title": "Critical Foundations (Review Daily)",
    "section": "2.3 Row Space",
    "text": "2.3 Row Space\n\nDefinition: Span of the row vectors of a matrix."
  },
  {
    "objectID": "exam3/daily_review.html#rank-nullity-theorem",
    "href": "exam3/daily_review.html#rank-nullity-theorem",
    "title": "Critical Foundations (Review Daily)",
    "section": "2.4 Rank-Nullity Theorem",
    "text": "2.4 Rank-Nullity Theorem\n\n\\text{Rank}(A) + \\text{Nullity}(A) = \\text{Number of columns of } A."
  },
  {
    "objectID": "exam3/daily_review.html#matrix-multiplication",
    "href": "exam3/daily_review.html#matrix-multiplication",
    "title": "Critical Foundations (Review Daily)",
    "section": "3.1 Matrix Multiplication",
    "text": "3.1 Matrix Multiplication\n\nAssociative but not commutative. Common mistake: Mixing these properties."
  },
  {
    "objectID": "exam3/daily_review.html#linear-transformations",
    "href": "exam3/daily_review.html#linear-transformations",
    "title": "Critical Foundations (Review Daily)",
    "section": "3.2 Linear Transformations",
    "text": "3.2 Linear Transformations\n\nDefined by matrix actions T(\\mathbf{x}) = A\\mathbf{x}."
  },
  {
    "objectID": "exam3/daily_review.html#elementary-row-operations",
    "href": "exam3/daily_review.html#elementary-row-operations",
    "title": "Critical Foundations (Review Daily)",
    "section": "3.3 Elementary Row Operations",
    "text": "3.3 Elementary Row Operations\n\nUsed for Gaussian elimination and finding RREF."
  },
  {
    "objectID": "exam3/daily_review.html#row-reduction-rref",
    "href": "exam3/daily_review.html#row-reduction-rref",
    "title": "Critical Foundations (Review Daily)",
    "section": "3.4 Row Reduction (RREF)",
    "text": "3.4 Row Reduction (RREF)\n\nSteps: Eliminate lower triangle, scale pivots, eliminate above pivots."
  },
  {
    "objectID": "exam3/daily_review.html#practice-problems",
    "href": "exam3/daily_review.html#practice-problems",
    "title": "Critical Foundations (Review Daily)",
    "section": "3.5 Practice Problems",
    "text": "3.5 Practice Problems\n\nCompute the null space and column space of a given matrix.\nProve that a given set is a subspace.\nFind a basis and dimension for a vector space.\n\n\nImportant Applications →"
  },
  {
    "objectID": "exam3/notecard_topics.html",
    "href": "exam3/notecard_topics.html",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "Vector Space: Set V with addition and scalar multiplication\nSubspace: Subset closed under operations, contains zero vector\nBasis: Linearly independent set spanning V\nDimension: Number of basis vectors\n\n\n\n\n\n\n\nVectors with no linear combinations of each other\nMinimal set that spans vector space\n\n\n\n\n\nRow Space: Span of row vectors\nColumn Space: Span of column vectors\nNull Space: Solutions to A\\mathbf{x} = \\mathbf{0}\n\n\n\n\n\n\nRank-Nullity: \\text{rank} + \\text{nullity} = n\nRank = \\dim(\\text{row space}) = \\dim(\\text{column space})\nNullity = \\dim(\\text{null space})\n\n\n\n\n\n\\det A \\ne 0\nLinearly independent columns\n\\text{Rank} = n\n\n\n\n\n\nBasis spans entire vector space\nZero vector in all vector spaces\nEmpty set ≠ vector space\n\n\n\n\n\nOne-to-One: Injective\nOnto: Surjective\nPreserves linear combinations\n\n\n\n\n\nRank: Image space dimension\nNullity: Kernel space dimension\nRow operations preserve row space\n\n\\det (determinant) is a scalar value computed for square matrices that provides critical information about the matrix’s properties:\nKey Determinant Insights:\n\nIndicates linear transformation’s volume scaling factor\n\\det A = 0 means columns/rows are linearly dependent\nSignifies matrix invertibility\nMeasures matrix’s “stretching” or “compressing” effect\n\nPractical Implications:\n\n\\det = 0: Non-invertible matrix\n\\det \\ne 0: Invertible matrix\nLarger absolute value means more significant volume transformation\n\nIn vector spaces, determinant helps assess:\n\nLinear independence\nTransformation characteristics\nMatrix invertibility"
  },
  {
    "objectID": "exam3/notecard_topics.html#key-definitions",
    "href": "exam3/notecard_topics.html#key-definitions",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "Vector Space: Set V with addition and scalar multiplication\nSubspace: Subset closed under operations, contains zero vector\nBasis: Linearly independent set spanning V\nDimension: Number of basis vectors"
  },
  {
    "objectID": "exam3/notecard_topics.html#core-concepts",
    "href": "exam3/notecard_topics.html#core-concepts",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "Vectors with no linear combinations of each other\nMinimal set that spans vector space\n\n\n\n\n\nRow Space: Span of row vectors\nColumn Space: Span of column vectors\nNull Space: Solutions to A\\mathbf{x} = \\mathbf{0}"
  },
  {
    "objectID": "exam3/notecard_topics.html#fundamental-theorems",
    "href": "exam3/notecard_topics.html#fundamental-theorems",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "Rank-Nullity: \\text{rank} + \\text{nullity} = n\nRank = \\dim(\\text{row space}) = \\dim(\\text{column space})\nNullity = \\dim(\\text{null space})"
  },
  {
    "objectID": "exam3/notecard_topics.html#invertibility-criteria",
    "href": "exam3/notecard_topics.html#invertibility-criteria",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "\\det A \\ne 0\nLinearly independent columns\n\\text{Rank} = n"
  },
  {
    "objectID": "exam3/notecard_topics.html#critical-relationships",
    "href": "exam3/notecard_topics.html#critical-relationships",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "Basis spans entire vector space\nZero vector in all vector spaces\nEmpty set ≠ vector space"
  },
  {
    "objectID": "exam3/notecard_topics.html#transformations",
    "href": "exam3/notecard_topics.html#transformations",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "One-to-One: Injective\nOnto: Surjective\nPreserves linear combinations"
  },
  {
    "objectID": "exam3/notecard_topics.html#key-takeaways",
    "href": "exam3/notecard_topics.html#key-takeaways",
    "title": "3 X 5 - Notecard",
    "section": "",
    "text": "Rank: Image space dimension\nNullity: Kernel space dimension\nRow operations preserve row space\n\n\\det (determinant) is a scalar value computed for square matrices that provides critical information about the matrix’s properties:\nKey Determinant Insights:\n\nIndicates linear transformation’s volume scaling factor\n\\det A = 0 means columns/rows are linearly dependent\nSignifies matrix invertibility\nMeasures matrix’s “stretching” or “compressing” effect\n\nPractical Implications:\n\n\\det = 0: Non-invertible matrix\n\\det \\ne 0: Invertible matrix\nLarger absolute value means more significant volume transformation\n\nIn vector spaces, determinant helps assess:\n\nLinear independence\nTransformation characteristics\nMatrix invertibility"
  },
  {
    "objectID": "exam4/chapter6_1.html",
    "href": "exam4/chapter6_1.html",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Chapter 6.1 introduces the fundamental concepts of inner products in \\mathbb{R}^n, which generalize the dot product to more abstract vector spaces. These ideas lay the groundwork for understanding vector length (norm), the notion of distance between vectors, and the concept of orthogonality. These are essential building blocks for deeper topics such as orthogonal projections, orthonormal bases, and decomposition of vector spaces into complementary subspaces.\nKey Ideas:\n\nInner Product: Generalization of the dot product that introduces notions of angle and length.\nLength (Norm): A measure of the size of a vector.\nDistance: A natural extension of the norm to measure how far apart two vectors are.\nOrthogonality: When two vectors are perpendicular, their inner product is zero. Orthogonality underlies many powerful results and techniques in linear algebra.\n\nBelow, we will cover each concept in detail, provide worked examples, and illustrate how they connect to fundamental spaces (column spaces, null spaces) and their applications (like orthogonal projection).\n\n\nInner ProductLength (Norm)DistanceOrthogonalityOrthogonality of MatricesConcluding Remarks\n\n\n\n\nDefinition of Inner Product (Dot Product in \\mathbb{R}^n):\nFor vectors \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n, the dot product is: \n\\mathbf{u} \\cdot \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n.\n\nProperties: For any \\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n and scalar c:\n\nCommutativity: \\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}\nDistributivity over Addition: (\\mathbf{u} + \\mathbf{v}) \\cdot \\mathbf{w} = \\mathbf{u} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{w}\nCompatibility with Scalar Multiplication: (c \\mathbf{u}) \\cdot \\mathbf{v} = c (\\mathbf{u} \\cdot \\mathbf{v}) = \\mathbf{u} \\cdot (c \\mathbf{v})\nPositivity: \\mathbf{u} \\cdot \\mathbf{u} \\geq 0, with equality if and only if \\mathbf{u} = \\mathbf{0}.\n\nRelation to Vector Spaces:\n\nThe inner product allows one to define angles between vectors, orthogonality, and lengths.\n\nA vector space equipped with an inner product is called an inner product space, and these form the foundation for more advanced constructs (e.g., Hilbert spaces in functional analysis).\n\n\n\n\nExample:\nFor \\mathbf{u} = [1, 2, 3] and \\mathbf{v} = [4, 5, 6], the dot product is: \n\\mathbf{u} \\cdot \\mathbf{v} = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32.\n\nProblem 1:\nGiven \\mathbf{u} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}:\n\n\n\\mathbf{u} \\cdot \\mathbf{u} = (-1)(-1) + (2)(2) = 1 + 4 = \\boxed{5}\n\n\n\\mathbf{v} \\cdot \\mathbf{u} = (4)(-1) + (6)(2) = -4 + 12 = \\boxed{8}\n\n\n\\dfrac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} = \\dfrac{8}{5} = \\boxed{\\tfrac{8}{5}}\n\n\n\n\n\n\nDefinition:\nIf \\mathbf{v} = (v_1, v_2, \\dots, v_n), then the length (or norm) of \\mathbf{v} is: \n\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n Also, \\|\\mathbf{v}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v}.\nRelation to Vector Spaces:\n- Norms measure the “size” of vectors and allow one to quantify concepts like “small,” “large,” or “close to zero.” - Norms are essential when constructing orthonormal bases, where each vector has length 1. - Norms also help describe null spaces: if \\mathbf{u} is in the null space of A, then \\|A\\mathbf{u}\\|=0, reflecting the idea that A\\mathbf{u} collapses that vector to the zero vector.\n\n\nExample:\nFor \\mathbf{u} = [3, 4]: \n\\|\\mathbf{u}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5.\n\nProblem 7:\nCompute the norm of \\mathbf{a} = \\begin{bmatrix} 3 \\\\ -1 \\\\ -5 \\end{bmatrix}: \n\\|\\mathbf{a}\\| = \\sqrt{3^2 + (-1)^2 + (-5)^2} = \\sqrt{9 + 1 + 25} = \\sqrt{35} = \\boxed{\\sqrt{35}}.\n\n\n\n\n\n\n\nThe distance between two vectors \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n is defined as: \nd(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}.\n\nThis naturally extends the concept of length to measure how far apart two vectors are from each other in space.\nRelation to Vector Spaces:\n\nDistance allows us to talk about convergence, continuity, and approximation within vector spaces.\nIn geometric interpretations, the distance from a vector to a subspace (like the column space of a matrix) is minimized by orthogonal projections.\n\nThe distance between a vector and the null space or column space helps describe how well a given system can approximate certain vectors.\n\n\n\n\n\n\n\n\n\n\nPolarization Identity/Expansion of the Squared Norm\n\n\n\n\n\nWe have identities for \\|\\mathbf{u}-\\mathbf{v}\\|^2 and \\|\\mathbf{u}+\\mathbf{v}\\|^2:\n\nStart with the squared norm: \n\\|\\mathbf{u} - \\mathbf{v}\\|^2 = (\\mathbf{u} - \\mathbf{v}) \\cdot (\\mathbf{u} - \\mathbf{v}).\n\nExpand: \n(\\mathbf{u} - \\mathbf{v}) \\cdot (\\mathbf{u} - \\mathbf{v}) = \\mathbf{u}\\cdot\\mathbf{u} - 2(\\mathbf{u}\\cdot\\mathbf{v}) + \\mathbf{v}\\cdot\\mathbf{v}.\n\nSimilarly: \n\\|\\mathbf{u} + \\mathbf{v}\\|^2 = (\\mathbf{u} + \\mathbf{v}) \\cdot (\\mathbf{u} + \\mathbf{v}) = \\mathbf{u}\\cdot\\mathbf{u} + 2(\\mathbf{u}\\cdot\\mathbf{v}) + \\mathbf{v}\\cdot\\mathbf{v}.\n\n\nThus: \n\\|\\mathbf{u} - \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2 - 2(\\mathbf{u}\\cdot\\mathbf{v}), \\\\\n\\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2 + 2(\\mathbf{u}\\cdot\\mathbf{v}).\n\nOrthogonality case: If \\mathbf{u}\\cdot\\mathbf{v}=0: \n\\|\\mathbf{u} \\pm \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2.\n\n\n\n\nExample:\nFor \\mathbf{u} = [1, 2] and \\mathbf{v} = [4, 6]: \n\\begin{aligned}\nd(\\mathbf{u}, \\mathbf{v}) &= \\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{(1 - 4)^2 + (2 - 6)^2} \\\\\n&= \\sqrt{(-3)^2 + (-4)^2} = \\sqrt{9 + 16} \\\\\n&= 5\n\\end{aligned}\n\n\n\n\n\n\n\nDefinition:\nVectors \\mathbf{u} and \\mathbf{v} are orthogonal if: \n\\mathbf{u} \\cdot \\mathbf{v} = 0.\n\nAn orthogonal set of vectors is a collection in which every pair of distinct vectors is mutually perpendicular (their dot product is zero).\nPythagorean Theorem in \\mathbb{R}^n:\nTwo vectors \\mathbf{u} and \\mathbf{v} are orthogonal if and only if: \n\\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2.\n\n\n\n\n\nOrthogonality underpins important decompositions of vector spaces.\n\nIf \\mathbf{u} is in the null space of A, it is orthogonal to every vector in the column space of A.\n\nMore generally, we have orthogonal complements: if V is a vector space, it can often be decomposed into the direct sum of a subspace and its orthogonal complement: \nV = \\text{Col}(A) \\oplus \\text{Nul}(A^T).\n\nOrthogonality is crucial for defining orthogonal projections, which “drop” a vector onto a subspace without introducing any component orthogonal to that subspace.\n\n\n\n\nExample:\nFor \\mathbf{u} = [1, 0] and \\mathbf{v} = [0, 1]: \n\\mathbf{u} \\cdot \\mathbf{v} = (1)(0) + (0)(1) = 0.\n These vectors are orthogonal (standard unit vectors along coordinate axes).\nProblem 28:\nIf \\mathbf{y} is orthogonal to both \\mathbf{u} and \\mathbf{v}, then \\mathbf{y} is orthogonal to every vector in \\text{Span}\\{\\mathbf{u}, \\mathbf{v}\\}.\nProof Outline: Any \\mathbf{w} in the span can be written as a\\mathbf{u}+b\\mathbf{v}. Since \\mathbf{y}\\cdot\\mathbf{u}=0 and \\mathbf{y}\\cdot\\mathbf{v}=0, it follows that \\mathbf{y}\\cdot\\mathbf{w}=0.\nProblem 5 (Projection):\nGiven \\mathbf{u} = \\begin{bmatrix}-1 \\\\ 2\\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix}4 \\\\ 6\\end{bmatrix}, compute: \n\\left(\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\mathbf{v}\\cdot\\mathbf{v}}\\right)\\mathbf{v}.\n\n\nWe found \\mathbf{v}\\cdot\\mathbf{v} = 52 and \\mathbf{u}\\cdot\\mathbf{v} = 8: \n\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\mathbf{v}\\cdot\\mathbf{v}} = \\frac{8}{52} = \\frac{2}{13}.\n Thus: \n\\left(\\frac{2}{13}\\right)\\mathbf{v} = \\begin{bmatrix}\\tfrac{8}{13}\\\\[6pt]\\tfrac{12}{13}\\end{bmatrix}.\n\n\nThis vector is the projection of \\mathbf{u} onto \\mathbf{v}, a fundamental concept in orthogonal decompositions.\n\n\n\n\n\n\nFor matrices, orthogonality is defined using an inner product known as the Frobenius inner product. For two matrices A and B of the same dimensions: \n\\langle A, B \\rangle_F = \\text{trace}(A^T B) = \\sum_{i,j} a_{ij}b_{ij}.\n\nDefinition:\nMatrices A and B are orthogonal (under the Frobenius inner product) if: \n\\langle A, B \\rangle_F = 0.\n\nRelation to Vector Spaces:\n- The concept of orthogonality extends from vectors to matrices by considering the space of all m\\times n matrices as a vector space over \\mathbb{R}.\n- Orthogonality of matrices is equivalent to the orthogonality of their vectorized forms (flattening the matrix into a long vector).\nExample:\nFor \nA = \\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix}, \\quad B = \\begin{bmatrix}0 & 0 \\\\ 0 & 1\\end{bmatrix},\n we have: \n\\langle A, B \\rangle_F = (1)(0) + (0)(0) + (0)(0) + (0)(1) = 0.\n\n\n\n\n\nIn this chapter, we have:\n\nIntroduced the concept of the inner product as a generalization of the familiar dot product, allowing for the definition of angles, lengths, and orthogonality in vector spaces.\nDefined the norm of a vector and showed how it leads to the notion of distance.\nExplored how orthogonality underpins many fundamental results in linear algebra, including projections, decompositions of vector spaces, and relationships between column spaces and null spaces.\nExtended the idea of inner products and orthogonality to matrices via the Frobenius inner product.\n\nThese concepts set the stage for more advanced topics like orthogonal projections, orthonormal bases, and least-squares solutions, all of which rely critically on the notions introduced here."
  },
  {
    "objectID": "exam4/chapter6_1.html#definition-properties",
    "href": "exam4/chapter6_1.html#definition-properties",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Definition of Inner Product (Dot Product in \\mathbb{R}^n):\nFor vectors \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n, the dot product is: \n\\mathbf{u} \\cdot \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n.\n\nProperties: For any \\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n and scalar c:\n\nCommutativity: \\mathbf{u} \\cdot \\mathbf{v} = \\mathbf{v} \\cdot \\mathbf{u}\nDistributivity over Addition: (\\mathbf{u} + \\mathbf{v}) \\cdot \\mathbf{w} = \\mathbf{u} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{w}\nCompatibility with Scalar Multiplication: (c \\mathbf{u}) \\cdot \\mathbf{v} = c (\\mathbf{u} \\cdot \\mathbf{v}) = \\mathbf{u} \\cdot (c \\mathbf{v})\nPositivity: \\mathbf{u} \\cdot \\mathbf{u} \\geq 0, with equality if and only if \\mathbf{u} = \\mathbf{0}.\n\nRelation to Vector Spaces:\n\nThe inner product allows one to define angles between vectors, orthogonality, and lengths.\n\nA vector space equipped with an inner product is called an inner product space, and these form the foundation for more advanced constructs (e.g., Hilbert spaces in functional analysis)."
  },
  {
    "objectID": "exam4/chapter6_1.html#examples-problems",
    "href": "exam4/chapter6_1.html#examples-problems",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Example:\nFor \\mathbf{u} = [1, 2, 3] and \\mathbf{v} = [4, 5, 6], the dot product is: \n\\mathbf{u} \\cdot \\mathbf{v} = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32.\n\nProblem 1:\nGiven \\mathbf{u} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}:\n\n\n\\mathbf{u} \\cdot \\mathbf{u} = (-1)(-1) + (2)(2) = 1 + 4 = \\boxed{5}\n\n\n\\mathbf{v} \\cdot \\mathbf{u} = (4)(-1) + (6)(2) = -4 + 12 = \\boxed{8}\n\n\n\\dfrac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} = \\dfrac{8}{5} = \\boxed{\\tfrac{8}{5}}"
  },
  {
    "objectID": "exam4/chapter6_1.html#examples-problems-1",
    "href": "exam4/chapter6_1.html#examples-problems-1",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Example:\nFor \\mathbf{u} = [3, 4]: \n\\|\\mathbf{u}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5.\n\nProblem 7:\nCompute the norm of \\mathbf{a} = \\begin{bmatrix} 3 \\\\ -1 \\\\ -5 \\end{bmatrix}: \n\\|\\mathbf{a}\\| = \\sqrt{3^2 + (-1)^2 + (-5)^2} = \\sqrt{9 + 1 + 25} = \\sqrt{35} = \\boxed{\\sqrt{35}}."
  },
  {
    "objectID": "exam4/chapter6_1.html#definition",
    "href": "exam4/chapter6_1.html#definition",
    "title": "Chapter 6.1",
    "section": "",
    "text": "The distance between two vectors \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n is defined as: \nd(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}.\n\nThis naturally extends the concept of length to measure how far apart two vectors are from each other in space.\nRelation to Vector Spaces:\n\nDistance allows us to talk about convergence, continuity, and approximation within vector spaces.\nIn geometric interpretations, the distance from a vector to a subspace (like the column space of a matrix) is minimized by orthogonal projections.\n\nThe distance between a vector and the null space or column space helps describe how well a given system can approximate certain vectors."
  },
  {
    "objectID": "exam4/chapter6_1.html#polarization-identity-expansion-of-the-squared-norm",
    "href": "exam4/chapter6_1.html#polarization-identity-expansion-of-the-squared-norm",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Polarization Identity/Expansion of the Squared Norm\n\n\n\n\n\nWe have identities for \\|\\mathbf{u}-\\mathbf{v}\\|^2 and \\|\\mathbf{u}+\\mathbf{v}\\|^2:\n\nStart with the squared norm: \n\\|\\mathbf{u} - \\mathbf{v}\\|^2 = (\\mathbf{u} - \\mathbf{v}) \\cdot (\\mathbf{u} - \\mathbf{v}).\n\nExpand: \n(\\mathbf{u} - \\mathbf{v}) \\cdot (\\mathbf{u} - \\mathbf{v}) = \\mathbf{u}\\cdot\\mathbf{u} - 2(\\mathbf{u}\\cdot\\mathbf{v}) + \\mathbf{v}\\cdot\\mathbf{v}.\n\nSimilarly: \n\\|\\mathbf{u} + \\mathbf{v}\\|^2 = (\\mathbf{u} + \\mathbf{v}) \\cdot (\\mathbf{u} + \\mathbf{v}) = \\mathbf{u}\\cdot\\mathbf{u} + 2(\\mathbf{u}\\cdot\\mathbf{v}) + \\mathbf{v}\\cdot\\mathbf{v}.\n\n\nThus: \n\\|\\mathbf{u} - \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2 - 2(\\mathbf{u}\\cdot\\mathbf{v}), \\\\\n\\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2 + 2(\\mathbf{u}\\cdot\\mathbf{v}).\n\nOrthogonality case: If \\mathbf{u}\\cdot\\mathbf{v}=0: \n\\|\\mathbf{u} \\pm \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2.\n\n\n\n\nExample:\nFor \\mathbf{u} = [1, 2] and \\mathbf{v} = [4, 6]: \n\\begin{aligned}\nd(\\mathbf{u}, \\mathbf{v}) &= \\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{(1 - 4)^2 + (2 - 6)^2} \\\\\n&= \\sqrt{(-3)^2 + (-4)^2} = \\sqrt{9 + 16} \\\\\n&= 5\n\\end{aligned}"
  },
  {
    "objectID": "exam4/chapter6_1.html#definition-key-ideas",
    "href": "exam4/chapter6_1.html#definition-key-ideas",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Definition:\nVectors \\mathbf{u} and \\mathbf{v} are orthogonal if: \n\\mathbf{u} \\cdot \\mathbf{v} = 0.\n\nAn orthogonal set of vectors is a collection in which every pair of distinct vectors is mutually perpendicular (their dot product is zero).\nPythagorean Theorem in \\mathbb{R}^n:\nTwo vectors \\mathbf{u} and \\mathbf{v} are orthogonal if and only if: \n\\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2."
  },
  {
    "objectID": "exam4/chapter6_1.html#relation-to-vector-spaces",
    "href": "exam4/chapter6_1.html#relation-to-vector-spaces",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Orthogonality underpins important decompositions of vector spaces.\n\nIf \\mathbf{u} is in the null space of A, it is orthogonal to every vector in the column space of A.\n\nMore generally, we have orthogonal complements: if V is a vector space, it can often be decomposed into the direct sum of a subspace and its orthogonal complement: \nV = \\text{Col}(A) \\oplus \\text{Nul}(A^T).\n\nOrthogonality is crucial for defining orthogonal projections, which “drop” a vector onto a subspace without introducing any component orthogonal to that subspace."
  },
  {
    "objectID": "exam4/chapter6_1.html#examples-problems-2",
    "href": "exam4/chapter6_1.html#examples-problems-2",
    "title": "Chapter 6.1",
    "section": "",
    "text": "Example:\nFor \\mathbf{u} = [1, 0] and \\mathbf{v} = [0, 1]: \n\\mathbf{u} \\cdot \\mathbf{v} = (1)(0) + (0)(1) = 0.\n These vectors are orthogonal (standard unit vectors along coordinate axes).\nProblem 28:\nIf \\mathbf{y} is orthogonal to both \\mathbf{u} and \\mathbf{v}, then \\mathbf{y} is orthogonal to every vector in \\text{Span}\\{\\mathbf{u}, \\mathbf{v}\\}.\nProof Outline: Any \\mathbf{w} in the span can be written as a\\mathbf{u}+b\\mathbf{v}. Since \\mathbf{y}\\cdot\\mathbf{u}=0 and \\mathbf{y}\\cdot\\mathbf{v}=0, it follows that \\mathbf{y}\\cdot\\mathbf{w}=0.\nProblem 5 (Projection):\nGiven \\mathbf{u} = \\begin{bmatrix}-1 \\\\ 2\\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix}4 \\\\ 6\\end{bmatrix}, compute: \n\\left(\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\mathbf{v}\\cdot\\mathbf{v}}\\right)\\mathbf{v}.\n\n\nWe found \\mathbf{v}\\cdot\\mathbf{v} = 52 and \\mathbf{u}\\cdot\\mathbf{v} = 8: \n\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\mathbf{v}\\cdot\\mathbf{v}} = \\frac{8}{52} = \\frac{2}{13}.\n Thus: \n\\left(\\frac{2}{13}\\right)\\mathbf{v} = \\begin{bmatrix}\\tfrac{8}{13}\\\\[6pt]\\tfrac{12}{13}\\end{bmatrix}.\n\n\nThis vector is the projection of \\mathbf{u} onto \\mathbf{v}, a fundamental concept in orthogonal decompositions."
  },
  {
    "objectID": "exam4/chapter6_1.html#definition-frobenius-inner-product",
    "href": "exam4/chapter6_1.html#definition-frobenius-inner-product",
    "title": "Chapter 6.1",
    "section": "",
    "text": "For matrices, orthogonality is defined using an inner product known as the Frobenius inner product. For two matrices A and B of the same dimensions: \n\\langle A, B \\rangle_F = \\text{trace}(A^T B) = \\sum_{i,j} a_{ij}b_{ij}.\n\nDefinition:\nMatrices A and B are orthogonal (under the Frobenius inner product) if: \n\\langle A, B \\rangle_F = 0.\n\nRelation to Vector Spaces:\n- The concept of orthogonality extends from vectors to matrices by considering the space of all m\\times n matrices as a vector space over \\mathbb{R}.\n- Orthogonality of matrices is equivalent to the orthogonality of their vectorized forms (flattening the matrix into a long vector).\nExample:\nFor \nA = \\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix}, \\quad B = \\begin{bmatrix}0 & 0 \\\\ 0 & 1\\end{bmatrix},\n we have: \n\\langle A, B \\rangle_F = (1)(0) + (0)(0) + (0)(0) + (0)(1) = 0."
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html",
    "href": "general_topics/eigenvalues_eigenvectors.html",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "Eigenvalues and eigenvectors are fundamental concepts in linear algebra that help us understand how linear transformations affect vector spaces. They are particularly useful in analyzing linear operators and their behavior on invariant subspaces.\nI’ll expand your request with collapsible example sections for each concept, showing detailed step-by-step calculations.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: A scalar \\lambda \\in \\mathbb{R} is called an eigenvalue of operator T: V \\to V if there exists a nonzero vector \\mathbf{u} \\in V such that T\\mathbf{u} = \\lambda\\mathbf{u}.\n\n\n\n\nExample: Finding Eigenvalues\n\nLet’s find the eigenvalues of matrix A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\nSet up characteristic equation: det(A - \\lambda I) = 0\nExpand determinant: det\\begin{bmatrix} 3-\\lambda & 1 \\\\ 1 & 3-\\lambda \\end{bmatrix} = 0 (3-\\lambda)(3-\\lambda) - (1)(1) = 0\nSolve polynomial: \\lambda^2 - 6\\lambda + 8 = 0 (\\lambda - 4)(\\lambda - 2) = 0 \\lambda = 4 \\text{ or } \\lambda = 2\n\nTherefore, the eigenvalues are \\lambda_1 = 4 and \\lambda_2 = 2\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For an eigenvalue \\lambda of matrix A, a vector \\mathbf{x} \\in V is called an eigenvector of A (corresponding to \\lambda) if A\\mathbf{x} = \\lambda\\mathbf{x}.\n\n\n\n\nExample: Finding Eigenvectors\n\nUsing the same matrix A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} and \\lambda_1 = 4:\n\nSet up equation (A - \\lambda I)\\mathbf{x} = \\mathbf{0}: \\begin{bmatrix} 3-4 & 1 \\\\ 1 & 3-4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\nSimplify system of equations: \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\nSolve system: -x_1 + x_2 = 0 x_1 = x_2\nChoose a non-zero solution: Let x_1 = 1, then x_2 = 1\n\nTherefore, \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} is an eigenvector for \\lambda_1 = 4\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The eigenspace of matrix A corresponding to eigenvalue \\lambda is the set of all solutions to (A - \\lambda I)\\mathbf{x} = \\mathbf{0}, which is equivalent to \\text{Nul}(A - \\lambda I).\n\n\n\n\nExample: Finding Eigenspace\n\nFor the same matrix A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} and \\lambda_1 = 4:\n\nForm A - \\lambda I: A - 4I = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}\nFind general solution to (A - 4I)\\mathbf{x} = \\mathbf{0}: \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\nRow reduce to find basis:\n\nFrom earlier, we found x_1 = x_2\nLet t be a free parameter\n\nExpress eigenspace: E_4 = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\right\\} = \\left\\{t\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} : t \\in \\mathbb{R}\\right\\}\n\nThis is the complete eigenspace corresponding to \\lambda_1 = 4\n\n\n\n\n\n\n\nStatement: The eigenvalues of a triangular matrix are the entries on its main diagonal.\n\n\n\n\n\n\nProof\n\n\n\nLet T: V \\to V have an upper-triangular matrix A with respect to some basis \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}:\nA = \\begin{bmatrix}\n\\lambda_1 & * & \\cdots & * \\\\\n0 & \\lambda_2 & \\cdots & * \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{bmatrix}\nFor any \\lambda \\in \\mathbb{R}, A - \\lambda I has the form: A - \\lambda I = \\begin{bmatrix}\n\\lambda_1 - \\lambda & * & \\cdots & * \\\\\n0 & \\lambda_2 - \\lambda & \\cdots & * \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n - \\lambda\n\\end{bmatrix}\nA - \\lambda I is not invertible if and only if \\lambda equals one of the \\lambda_i’s (since \\det(A - \\lambda I) = \\prod_{i=1}^n (\\lambda_i - \\lambda)). Therefore, \\lambda is an eigenvalue of T if and only if \\lambda equals one of the diagonal entries \\lambda_i.\n\n\n\n\n\nStatement: If \\mathbf{v}_1, \\ldots, \\mathbf{v}_r are eigenvectors that correspond to distinct eigenvalues \\lambda_1, \\ldots, \\lambda_r of an n \\times n matrix A, then the set \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\} is linearly independent.\n\n\n\n\n\n\nProof\n\n\n\n\nAssume by contradiction that \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\} is linearly dependent\nLet k be smallest positive integer such that \\mathbf{v}_k \\in \\text{Span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{k-1}\\}\nThen there exist scalars a_1, \\ldots, a_{k-1} such that: \\mathbf{v}_k = a_1\\mathbf{v}_1 + \\cdots + a_{k-1}\\mathbf{v}_{k-1}\nApply A to both sides: \\lambda_k\\mathbf{v}_k = a_1\\lambda_1\\mathbf{v}_1 + \\cdots + a_{k-1}\\lambda_{k-1}\\mathbf{v}_{k-1}\nMultiply original equation by \\lambda_k and subtract: \\mathbf{0} = a_1(\\lambda_k - \\lambda_1)\\mathbf{v}_1 + \\cdots + a_{k-1}(\\lambda_k - \\lambda_{k-1})\\mathbf{v}_{k-1}\nSince \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{k-1}\\} is linearly independent and \\lambda_k \\neq \\lambda_i for i &lt; k, all a_i must be 0\nThis implies \\mathbf{v}_k = \\mathbf{0}, contradicting that \\mathbf{v}_k is an eigenvector Therefore, \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\} must be linearly independent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\n\n\nWhat does it mean for a matrix A to have an eigenvalue of 0?\n\n\nFor \\lambda = 0 to be an eigenvalue of A:\n\nThere must exist a nonzero vector \\mathbf{x} such that A\\mathbf{x} = 0\\mathbf{x} = \\mathbf{0}\nThis means \\mathbf{x} \\in \\text{Nul}(A), and \\mathbf{x} \\neq \\mathbf{0}\nTherefore, A having an eigenvalue of 0 means:\n\nA is not invertible\nA has a nontrivial null space\n\\det(A) = 0\nA is singular\n\n\nExample: Consider A = \\begin{bmatrix} 1 & 2 \\\\ -2 & -4 \\end{bmatrix} - \\det(A) = 1(-4) - 2(-2) = -4 + 4 = 0 - The vector \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} is in \\text{Nul}(A) - Therefore, 0 is an eigenvalue with eigenvector \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\n\n\nGiven a first-order difference equation \\mathbf{x}_{k+1} = A\\mathbf{x}_k, where A is an n\\times n matrix, explain how to construct solutions.\n\n\n\nFind eigenvalues \\lambda_i and corresponding eigenvectors \\mathbf{v}_i of A\nAny solution has the form: \\mathbf{x}_k = c_1\\lambda_1^k\\mathbf{v}_1 + c_2\\lambda_2^k\\mathbf{v}_2 + \\cdots + c_n\\lambda_n^k\\mathbf{v}_n where c_i are constants determined by initial conditions\n\nExample: For A = \\begin{bmatrix} 2 & 1 \\\\ -1 & 4 \\end{bmatrix}, let’s find solution with \\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}: 1. Find eigenvalues: \\lambda = 3,1 2. Find eigenvectors: \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} 3. Write \\mathbf{x}_0 = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 4. Solve for c_i: \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = c_1\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + c_2\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} 5. Get c_1 = \\frac{1}{2}, c_2 = \\frac{1}{2} 6. Solution: \\mathbf{x}_k = \\frac{1}{2}(3^k)\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + \\frac{1}{2}(1^k)\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\n\n\nDetermine if \\lambda = 2 is an eigenvalue of A = \\begin{bmatrix} 3 & 1 \\\\ -1 & 5 \\end{bmatrix}\n\n\nStep 1: Form A - 2I A - 2I = \\begin{bmatrix} 1 & 1 \\\\ -1 & 3 \\end{bmatrix}\nStep 2: Calculate \\det(A - 2I) \\det(A - 2I) = (1)(3) - (1)(-1) = 3 + 1 = 4\nStep 3: Conclusion Since \\det(A - 2I) \\neq 0, \\lambda = 2 is not an eigenvalue of A\nAlternative method: 1. Find characteristic equation: \\det(A - \\lambda I) = 0 2. (3-\\lambda)(5-\\lambda) - (-1)(1) = 0 3. \\lambda^2 - 8\\lambda + 14 = 0 4. \\lambda = 4 \\pm \\sqrt{2} Therefore, 2 is not an eigenvalue\n\n\n\n\n\n\n\n\n\nNot checking if eigenvector is nonzero\nAssuming eigenvalues must be on the main diagonal (only true for triangular matrices)\nForgetting that eigenvectors corresponding to the same eigenvalue might be linearly dependent\nNot verifying that λ is a real number when working in real vector spaces\nConfusing eigenspace with the span of eigenvectors\n\n\n\n\n\n3Blue1Brown - Essence of Linear Algebra (YouTube)\nGilbert Strang’s Linear Algebra Lectures (MIT OpenCourseWare)\n“Introduction to Probability” by Bertsekas and Tsitsiklis\n“Matrix Analysis” by Horn and Johnson"
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html#introduction",
    "href": "general_topics/eigenvalues_eigenvectors.html#introduction",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "Eigenvalues and eigenvectors are fundamental concepts in linear algebra that help us understand how linear transformations affect vector spaces. They are particularly useful in analyzing linear operators and their behavior on invariant subspaces.\nI’ll expand your request with collapsible example sections for each concept, showing detailed step-by-step calculations."
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html#key-concepts-and-definitions",
    "href": "general_topics/eigenvalues_eigenvectors.html#key-concepts-and-definitions",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "Note\n\n\n\nDefinition: A scalar \\lambda \\in \\mathbb{R} is called an eigenvalue of operator T: V \\to V if there exists a nonzero vector \\mathbf{u} \\in V such that T\\mathbf{u} = \\lambda\\mathbf{u}.\n\n\n\n\nExample: Finding Eigenvalues\n\nLet’s find the eigenvalues of matrix A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\n\nSet up characteristic equation: det(A - \\lambda I) = 0\nExpand determinant: det\\begin{bmatrix} 3-\\lambda & 1 \\\\ 1 & 3-\\lambda \\end{bmatrix} = 0 (3-\\lambda)(3-\\lambda) - (1)(1) = 0\nSolve polynomial: \\lambda^2 - 6\\lambda + 8 = 0 (\\lambda - 4)(\\lambda - 2) = 0 \\lambda = 4 \\text{ or } \\lambda = 2\n\nTherefore, the eigenvalues are \\lambda_1 = 4 and \\lambda_2 = 2\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For an eigenvalue \\lambda of matrix A, a vector \\mathbf{x} \\in V is called an eigenvector of A (corresponding to \\lambda) if A\\mathbf{x} = \\lambda\\mathbf{x}.\n\n\n\n\nExample: Finding Eigenvectors\n\nUsing the same matrix A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} and \\lambda_1 = 4:\n\nSet up equation (A - \\lambda I)\\mathbf{x} = \\mathbf{0}: \\begin{bmatrix} 3-4 & 1 \\\\ 1 & 3-4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\nSimplify system of equations: \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\nSolve system: -x_1 + x_2 = 0 x_1 = x_2\nChoose a non-zero solution: Let x_1 = 1, then x_2 = 1\n\nTherefore, \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} is an eigenvector for \\lambda_1 = 4\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The eigenspace of matrix A corresponding to eigenvalue \\lambda is the set of all solutions to (A - \\lambda I)\\mathbf{x} = \\mathbf{0}, which is equivalent to \\text{Nul}(A - \\lambda I).\n\n\n\n\nExample: Finding Eigenspace\n\nFor the same matrix A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} and \\lambda_1 = 4:\n\nForm A - \\lambda I: A - 4I = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}\nFind general solution to (A - 4I)\\mathbf{x} = \\mathbf{0}: \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\nRow reduce to find basis:\n\nFrom earlier, we found x_1 = x_2\nLet t be a free parameter\n\nExpress eigenspace: E_4 = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\right\\} = \\left\\{t\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} : t \\in \\mathbb{R}\\right\\}\n\nThis is the complete eigenspace corresponding to \\lambda_1 = 4"
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html#important-theorems",
    "href": "general_topics/eigenvalues_eigenvectors.html#important-theorems",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "Statement: The eigenvalues of a triangular matrix are the entries on its main diagonal.\n\n\n\n\n\n\nProof\n\n\n\nLet T: V \\to V have an upper-triangular matrix A with respect to some basis \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}:\nA = \\begin{bmatrix}\n\\lambda_1 & * & \\cdots & * \\\\\n0 & \\lambda_2 & \\cdots & * \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{bmatrix}\nFor any \\lambda \\in \\mathbb{R}, A - \\lambda I has the form: A - \\lambda I = \\begin{bmatrix}\n\\lambda_1 - \\lambda & * & \\cdots & * \\\\\n0 & \\lambda_2 - \\lambda & \\cdots & * \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n - \\lambda\n\\end{bmatrix}\nA - \\lambda I is not invertible if and only if \\lambda equals one of the \\lambda_i’s (since \\det(A - \\lambda I) = \\prod_{i=1}^n (\\lambda_i - \\lambda)). Therefore, \\lambda is an eigenvalue of T if and only if \\lambda equals one of the diagonal entries \\lambda_i.\n\n\n\n\n\nStatement: If \\mathbf{v}_1, \\ldots, \\mathbf{v}_r are eigenvectors that correspond to distinct eigenvalues \\lambda_1, \\ldots, \\lambda_r of an n \\times n matrix A, then the set \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\} is linearly independent.\n\n\n\n\n\n\nProof\n\n\n\n\nAssume by contradiction that \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\} is linearly dependent\nLet k be smallest positive integer such that \\mathbf{v}_k \\in \\text{Span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{k-1}\\}\nThen there exist scalars a_1, \\ldots, a_{k-1} such that: \\mathbf{v}_k = a_1\\mathbf{v}_1 + \\cdots + a_{k-1}\\mathbf{v}_{k-1}\nApply A to both sides: \\lambda_k\\mathbf{v}_k = a_1\\lambda_1\\mathbf{v}_1 + \\cdots + a_{k-1}\\lambda_{k-1}\\mathbf{v}_{k-1}\nMultiply original equation by \\lambda_k and subtract: \\mathbf{0} = a_1(\\lambda_k - \\lambda_1)\\mathbf{v}_1 + \\cdots + a_{k-1}(\\lambda_k - \\lambda_{k-1})\\mathbf{v}_{k-1}\nSince \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_{k-1}\\} is linearly independent and \\lambda_k \\neq \\lambda_i for i &lt; k, all a_i must be 0\nThis implies \\mathbf{v}_k = \\mathbf{0}, contradicting that \\mathbf{v}_k is an eigenvector Therefore, \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_r\\} must be linearly independent"
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html#practice-problems",
    "href": "general_topics/eigenvalues_eigenvectors.html#practice-problems",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "Problem\n\n\n\n\n\nWhat does it mean for a matrix A to have an eigenvalue of 0?\n\n\nFor \\lambda = 0 to be an eigenvalue of A:\n\nThere must exist a nonzero vector \\mathbf{x} such that A\\mathbf{x} = 0\\mathbf{x} = \\mathbf{0}\nThis means \\mathbf{x} \\in \\text{Nul}(A), and \\mathbf{x} \\neq \\mathbf{0}\nTherefore, A having an eigenvalue of 0 means:\n\nA is not invertible\nA has a nontrivial null space\n\\det(A) = 0\nA is singular\n\n\nExample: Consider A = \\begin{bmatrix} 1 & 2 \\\\ -2 & -4 \\end{bmatrix} - \\det(A) = 1(-4) - 2(-2) = -4 + 4 = 0 - The vector \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} is in \\text{Nul}(A) - Therefore, 0 is an eigenvalue with eigenvector \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\n\n\nGiven a first-order difference equation \\mathbf{x}_{k+1} = A\\mathbf{x}_k, where A is an n\\times n matrix, explain how to construct solutions.\n\n\n\nFind eigenvalues \\lambda_i and corresponding eigenvectors \\mathbf{v}_i of A\nAny solution has the form: \\mathbf{x}_k = c_1\\lambda_1^k\\mathbf{v}_1 + c_2\\lambda_2^k\\mathbf{v}_2 + \\cdots + c_n\\lambda_n^k\\mathbf{v}_n where c_i are constants determined by initial conditions\n\nExample: For A = \\begin{bmatrix} 2 & 1 \\\\ -1 & 4 \\end{bmatrix}, let’s find solution with \\mathbf{x}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}: 1. Find eigenvalues: \\lambda = 3,1 2. Find eigenvectors: \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} 3. Write \\mathbf{x}_0 = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 4. Solve for c_i: \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = c_1\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + c_2\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} 5. Get c_1 = \\frac{1}{2}, c_2 = \\frac{1}{2} 6. Solution: \\mathbf{x}_k = \\frac{1}{2}(3^k)\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + \\frac{1}{2}(1^k)\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\n\n\nDetermine if \\lambda = 2 is an eigenvalue of A = \\begin{bmatrix} 3 & 1 \\\\ -1 & 5 \\end{bmatrix}\n\n\nStep 1: Form A - 2I A - 2I = \\begin{bmatrix} 1 & 1 \\\\ -1 & 3 \\end{bmatrix}\nStep 2: Calculate \\det(A - 2I) \\det(A - 2I) = (1)(3) - (1)(-1) = 3 + 1 = 4\nStep 3: Conclusion Since \\det(A - 2I) \\neq 0, \\lambda = 2 is not an eigenvalue of A\nAlternative method: 1. Find characteristic equation: \\det(A - \\lambda I) = 0 2. (3-\\lambda)(5-\\lambda) - (-1)(1) = 0 3. \\lambda^2 - 8\\lambda + 14 = 0 4. \\lambda = 4 \\pm \\sqrt{2} Therefore, 2 is not an eigenvalue"
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html#common-mistakes-to-avoid",
    "href": "general_topics/eigenvalues_eigenvectors.html#common-mistakes-to-avoid",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "Not checking if eigenvector is nonzero\nAssuming eigenvalues must be on the main diagonal (only true for triangular matrices)\nForgetting that eigenvectors corresponding to the same eigenvalue might be linearly dependent\nNot verifying that λ is a real number when working in real vector spaces\nConfusing eigenspace with the span of eigenvectors"
  },
  {
    "objectID": "general_topics/eigenvalues_eigenvectors.html#additional-resources",
    "href": "general_topics/eigenvalues_eigenvectors.html#additional-resources",
    "title": "Eigenvalues and Eigenvectors in Linear Algebra",
    "section": "",
    "text": "3Blue1Brown - Essence of Linear Algebra (YouTube)\nGilbert Strang’s Linear Algebra Lectures (MIT OpenCourseWare)\n“Introduction to Probability” by Bertsekas and Tsitsiklis\n“Matrix Analysis” by Horn and Johnson"
  },
  {
    "objectID": "general_topics/linear_independence_and_span.html",
    "href": "general_topics/linear_independence_and_span.html",
    "title": "Linear Independence and Span",
    "section": "",
    "text": "Definition: A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. Mathematically, vectors \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n are independent if the equation c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n = 0 only has the trivial solution c_1 = c_2 = \\cdots = c_n = 0.\nImportance: Independence is crucial for defining bases, understanding dimension, and ensuring minimality in spanning sets.\nGeometric Interpretation: In \\mathbb{R}^3, three linearly independent vectors span the entire space, while fewer vectors (e.g., two) only span a subspace (e.g., a plane).\n\n\n\n\n\nDefinition: The span of a set of vectors is the collection of all linear combinations of those vectors. If vectors \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n span a space V, then any vector in V can be expressed as c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n for some scalars c_1, c_2, \\ldots, c_n.\nRelation to Vector Spaces: A spanning set that is also linearly independent forms a basis for the space, providing both coverage and minimality."
  },
  {
    "objectID": "general_topics/linear_independence_and_span.html#linear-independence",
    "href": "general_topics/linear_independence_and_span.html#linear-independence",
    "title": "Linear Independence and Span",
    "section": "",
    "text": "Definition: A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. Mathematically, vectors \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n are independent if the equation c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n = 0 only has the trivial solution c_1 = c_2 = \\cdots = c_n = 0.\nImportance: Independence is crucial for defining bases, understanding dimension, and ensuring minimality in spanning sets.\nGeometric Interpretation: In \\mathbb{R}^3, three linearly independent vectors span the entire space, while fewer vectors (e.g., two) only span a subspace (e.g., a plane)."
  },
  {
    "objectID": "general_topics/linear_independence_and_span.html#span-of-a-set-of-vectors",
    "href": "general_topics/linear_independence_and_span.html#span-of-a-set-of-vectors",
    "title": "Linear Independence and Span",
    "section": "",
    "text": "Definition: The span of a set of vectors is the collection of all linear combinations of those vectors. If vectors \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n span a space V, then any vector in V can be expressed as c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n for some scalars c_1, c_2, \\ldots, c_n.\nRelation to Vector Spaces: A spanning set that is also linearly independent forms a basis for the space, providing both coverage and minimality."
  },
  {
    "objectID": "general_topics/linear_independence_and_span.html#basic-examples",
    "href": "general_topics/linear_independence_and_span.html#basic-examples",
    "title": "Linear Independence and Span",
    "section": "3.1 Basic Examples",
    "text": "3.1 Basic Examples\n\n3.1.1 Testing for Linear Independence\nConsider vectors \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} and \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}. To check independence, we solve for scalars c_1 and c_2 such that: c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{0}. Substituting values, we get: c_1 \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + c_2 \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, leading to c_1 + 2c_2 = 0 and 2c_1 + 4c_2 = 0. These equations imply c_1 = -2c_2, meaning the only solution is trivial if c_1 = c_2 = 0, proving the vectors are dependent.\n\n\n3.1.2 Finding the Span\nFor vectors \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} and \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, their span is all of \\mathbb{R}^2 since any vector \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} can be written as x\\mathbf{v}_1 + y\\mathbf{v}_2."
  },
  {
    "objectID": "general_topics/linear_independence_and_span.html#advanced-examples",
    "href": "general_topics/linear_independence_and_span.html#advanced-examples",
    "title": "Linear Independence and Span",
    "section": "3.2 Advanced Examples",
    "text": "3.2 Advanced Examples\n\n3.2.1 Basis Selection\nIn \\mathbb{R}^3, find a basis for the subspace spanned by \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}, and \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}. Notice that the first two vectors are linearly dependent, so a basis can be \\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\right\\}."
  },
  {
    "objectID": "general_topics/null_spaces_col_spaces.html",
    "href": "general_topics/null_spaces_col_spaces.html",
    "title": "Null Space and Column Space",
    "section": "",
    "text": "Definition: The null space (or kernel) of a matrix A is the set of all vectors \\mathbf{x} such that A\\mathbf{x} = \\mathbf{0}. It represents solutions to the homogeneous equation, meaning all vectors that the matrix transforms to the zero vector.\nProperties:\n\nThe null space is a subspace of \\mathbb{R}^n (where n is the number of columns of A).\nThe dimension of the null space is known as the nullity of A.\n\nGeometric Interpretation: The null space represents directions in the domain that map to zero. For example, if a transformation represented by A compresses the plane into a line, the null space corresponds to the directions that get “flattened” in that process.\n\n\n\n\n\nDefinition: The column space (or range) of a matrix A is the span of its column vectors. It represents all possible linear combinations of the columns of A and, geometrically, the “reach” or “range” of the transformation applied by A.\nProperties:\n\nThe column space is a subspace of \\mathbb{R}^m (where m is the number of rows of A).\nThe dimension of the column space is called the rank of A.\n\nRelation to Linear Transformations: The column space represents the image of the transformation, describing all possible output vectors."
  },
  {
    "objectID": "general_topics/null_spaces_col_spaces.html#what-is-the-null-space",
    "href": "general_topics/null_spaces_col_spaces.html#what-is-the-null-space",
    "title": "Null Space and Column Space",
    "section": "",
    "text": "Definition: The null space (or kernel) of a matrix A is the set of all vectors \\mathbf{x} such that A\\mathbf{x} = \\mathbf{0}. It represents solutions to the homogeneous equation, meaning all vectors that the matrix transforms to the zero vector.\nProperties:\n\nThe null space is a subspace of \\mathbb{R}^n (where n is the number of columns of A).\nThe dimension of the null space is known as the nullity of A.\n\nGeometric Interpretation: The null space represents directions in the domain that map to zero. For example, if a transformation represented by A compresses the plane into a line, the null space corresponds to the directions that get “flattened” in that process."
  },
  {
    "objectID": "general_topics/null_spaces_col_spaces.html#what-is-the-column-space",
    "href": "general_topics/null_spaces_col_spaces.html#what-is-the-column-space",
    "title": "Null Space and Column Space",
    "section": "",
    "text": "Definition: The column space (or range) of a matrix A is the span of its column vectors. It represents all possible linear combinations of the columns of A and, geometrically, the “reach” or “range” of the transformation applied by A.\nProperties:\n\nThe column space is a subspace of \\mathbb{R}^m (where m is the number of rows of A).\nThe dimension of the column space is called the rank of A.\n\nRelation to Linear Transformations: The column space represents the image of the transformation, describing all possible output vectors."
  },
  {
    "objectID": "general_topics/null_spaces_col_spaces.html#basic-examples",
    "href": "general_topics/null_spaces_col_spaces.html#basic-examples",
    "title": "Null Space and Column Space",
    "section": "3.1 Basic Examples",
    "text": "3.1 Basic Examples\n\n3.1.1 Finding the Null Space of a 2×2 Matrix\nLet A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}. To find the null space, we solve A\\mathbf{x} = \\mathbf{0}: \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}. This system reduces to: x_1 + 2x_2 = 0. Thus, x_1 = -2x_2, so any solution is a scalar multiple of \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}. The null space is therefore: \\text{Null}(A) = \\text{span}\\left\\{ \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix} \\right\\}.\n\n\n3.1.2 Finding the Column Space\nThe column space of A is the span of its columns: \\text{Col}(A) = \\text{span}\\left\\{ \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 6 \\end{bmatrix} \\right\\}. Since the second column is a scalar multiple of the first, the column space is just the span of \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}."
  },
  {
    "objectID": "general_topics/null_spaces_col_spaces.html#advanced-examples",
    "href": "general_topics/null_spaces_col_spaces.html#advanced-examples",
    "title": "Null Space and Column Space",
    "section": "3.2 Advanced Examples",
    "text": "3.2 Advanced Examples\n\n3.2.1 Real-World Application: Signal Processing\nIn signal processing, the null space is essential in understanding the solutions of systems affected by noise or interference. By projecting signals into the null space, unwanted signals or noise can be minimized."
  },
  {
    "objectID": "general_topics/rank.html",
    "href": "general_topics/rank.html",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "The concept of rank is central to linear algebra and provides insight into the structure and properties of matrices, particularly in the context of solutions to systems of linear equations. Rank helps define dimensions of subspaces formed by the matrix rows or columns and plays a role in determining matrix invertibility.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: For a matrix A, the row space is the set of all linear combinations of its rows, and the column space is the set of all linear combinations of its columns. These spaces represent the span of the row and column vectors and define the respective dimensions of the space. For any matrix A, if we denote the row space by Row(A) and the column space by Col(A), these spaces are essential in defining the matrix’s rank.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The rank of a matrix A, denoted \\text{rank}(A), is the dimension of its column space (or equivalently, the row space). This dimension is also equal to the number of pivot positions in the matrix when it is in echelon form.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The null space of a matrix A, denoted Nul(A), is the set of all vectors x such that A \\cdot x = 0. The nullity of A is the dimension of the null space and is related to the rank by the Rank-Nullity Theorem.\n\n\n\n\n\n\nRow operations do not change the row space or rank of a matrix. Thus, row-equivalent matrices share the same row space and rank. This property is particularly useful for transforming matrices to row echelon form or reduced row echelon form to determine rank.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor an m \\times n matrix A, the rank theorem states that:\n\\text{rank}(A) + \\text{dim Nul}(A) = n\nThis relationship shows that the sum of the dimensions of the column space and null space equals the number of columns in the matrix. This theorem is fundamental for analyzing solutions to linear systems.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Invertible Matrix Theorem provides several conditions that are equivalent to the invertibility of an n \\times n matrix A. Among these are the rank conditions:\n\nThe columns of A form a basis for \\mathbb{R}^n.\n\\text{Col}(A) = \\mathbb{R}^n.\n\\text{dim Col}(A) = n.\n\\text{rank}(A) = n.\n\\text{Nul}(A) = \\{0\\}.\n\\text{dim Nul}(A) = 0.\n\n\n\n\n\n\n\n\nUnique Solution: If the rank of the coefficient matrix equals the number of variables, and the system is consistent, there is a unique solution.\nInfinitely Many Solutions: If the rank of the matrix is less than the number of variables, there are infinitely many solutions.\nNo Solution: If the rank of the augmented matrix is greater than the rank of the coefficient matrix, the system is inconsistent.\n\n\n\n\n\n\n\nFull Row Rank: For a matrix with rank equal to the number of rows (full row rank), every row is linearly independent.\nFull Column Rank: For a matrix with rank equal to the number of columns (full column rank), every column is linearly independent, implying invertibility for square matrices.\n\n\n\n\nFor a square matrix A:\n\nIf \\text{det}(A) \\neq 0, then \\text{rank}(A) = n (full rank), and A is invertible.\nIf \\text{det}(A) = 0, then \\text{rank}(A) &lt; n, and A is singular (non-invertible).\n\nHere’s the updated section for your Quarto study sheet with the new problems and their solutions:\n\n\n\n\n\n\n\nFor a 6 \\times 3 matrix A with rank 3, determine:\n\n\\text{dim Nul}(A)\n\\text{dim Row}(A)\n\\text{rank}(A^T)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\text{dim Nul}(A) = 0 since \\text{rank}(A) = 3.\n\\text{dim Row}(A) = 3.\n\\text{rank}(A^T) = 3, as rank remains the same under transposition.\n\n\n\n\n\n\n\nIf the null space of a 7 \\times 6 matrix A is 5-dimensional, what is \\text{dim Col}(A)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the Rank-Nullity Theorem: text{rank}(A) + \\text{dim Nul}(A) = 6 Since \\text{dim Nul}(A) = 5, \\text{rank}(A) = 1, so \\text{dim Col}(A) = 1.\n\n\n\n\n\n\nIf A is a 6 \\times 4 matrix, what is the smallest possible dimension of Nul(A)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe smallest possible dimension of Nul(A) is 0 if \\text{rank}(A) = 4, meaning the matrix has full column rank.\n\n\n\n\n\n\nSuppose the solutions of a homogeneous system of five linear equations in six unknowns are all multiples of one nonzero solution. Will the system necessarily have a solution for every possible choice of constants on the right sides of the equations? Explain.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe system does not necessarily have a solution for every possible choice of constants. The key observation is that the homogeneous system has a null space of dimension 1, as all solutions are multiples of one nonzero solution. For the inhomogeneous system to have a solution for every choice of constants, the coefficient matrix must have full row rank (rank 5). However, since there are 6 unknowns, the rank cannot exceed 5, leaving one degree of freedom in the solution space. This means there may exist some choices of constants for which the system is inconsistent.\n\n\n\n\n\n\nIn statistical theory, a common requirement is that a matrix be of full rank. That is, the rank should be as large as possible. Explain why an m \\times n matrix with more rows than columns has full rank if and only if its columns are linearly independent.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAn m \\times n matrix A (where m &gt; n) has full rank when \\text{rank}(A) = n, which implies that the columns span an n-dimensional space. This is only possible if the columns are linearly independent; otherwise, the rank would be less than n. If the columns are linearly dependent, at least one column can be expressed as a linear combination of others, reducing the rank. Therefore, full rank requires that the columns be linearly independent.\n\n\n\n\n\n\n\n\n\n3Blue1Brown - Essence of Linear Algebra (YouTube)\nKhan Academy - Linear Algebra (YouTube)\nMIT OpenCourseWare - Linear Algebra"
  },
  {
    "objectID": "general_topics/rank.html#introduction",
    "href": "general_topics/rank.html#introduction",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "The concept of rank is central to linear algebra and provides insight into the structure and properties of matrices, particularly in the context of solutions to systems of linear equations. Rank helps define dimensions of subspaces formed by the matrix rows or columns and plays a role in determining matrix invertibility."
  },
  {
    "objectID": "general_topics/rank.html#key-concepts-and-definitions",
    "href": "general_topics/rank.html#key-concepts-and-definitions",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "Note\n\n\n\nDefinition: For a matrix A, the row space is the set of all linear combinations of its rows, and the column space is the set of all linear combinations of its columns. These spaces represent the span of the row and column vectors and define the respective dimensions of the space. For any matrix A, if we denote the row space by Row(A) and the column space by Col(A), these spaces are essential in defining the matrix’s rank.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The rank of a matrix A, denoted \\text{rank}(A), is the dimension of its column space (or equivalently, the row space). This dimension is also equal to the number of pivot positions in the matrix when it is in echelon form.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefinition: The null space of a matrix A, denoted Nul(A), is the set of all vectors x such that A \\cdot x = 0. The nullity of A is the dimension of the null space and is related to the rank by the Rank-Nullity Theorem."
  },
  {
    "objectID": "general_topics/rank.html#effects-of-row-operations-on-rank",
    "href": "general_topics/rank.html#effects-of-row-operations-on-rank",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "Row operations do not change the row space or rank of a matrix. Thus, row-equivalent matrices share the same row space and rank. This property is particularly useful for transforming matrices to row echelon form or reduced row echelon form to determine rank."
  },
  {
    "objectID": "general_topics/rank.html#important-theorems",
    "href": "general_topics/rank.html#important-theorems",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "Tip\n\n\n\nFor an m \\times n matrix A, the rank theorem states that:\n\\text{rank}(A) + \\text{dim Nul}(A) = n\nThis relationship shows that the sum of the dimensions of the column space and null space equals the number of columns in the matrix. This theorem is fundamental for analyzing solutions to linear systems.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Invertible Matrix Theorem provides several conditions that are equivalent to the invertibility of an n \\times n matrix A. Among these are the rank conditions:\n\nThe columns of A form a basis for \\mathbb{R}^n.\n\\text{Col}(A) = \\mathbb{R}^n.\n\\text{dim Col}(A) = n.\n\\text{rank}(A) = n.\n\\text{Nul}(A) = \\{0\\}.\n\\text{dim Nul}(A) = 0."
  },
  {
    "objectID": "general_topics/rank.html#relationship-between-rank-and-solutions-to-linear-systems",
    "href": "general_topics/rank.html#relationship-between-rank-and-solutions-to-linear-systems",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "Unique Solution: If the rank of the coefficient matrix equals the number of variables, and the system is consistent, there is a unique solution.\nInfinitely Many Solutions: If the rank of the matrix is less than the number of variables, there are infinitely many solutions.\nNo Solution: If the rank of the augmented matrix is greater than the rank of the coefficient matrix, the system is inconsistent."
  },
  {
    "objectID": "general_topics/rank.html#special-cases-or-applications",
    "href": "general_topics/rank.html#special-cases-or-applications",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "Full Row Rank: For a matrix with rank equal to the number of rows (full row rank), every row is linearly independent.\nFull Column Rank: For a matrix with rank equal to the number of columns (full column rank), every column is linearly independent, implying invertibility for square matrices.\n\n\n\n\nFor a square matrix A:\n\nIf \\text{det}(A) \\neq 0, then \\text{rank}(A) = n (full rank), and A is invertible.\nIf \\text{det}(A) = 0, then \\text{rank}(A) &lt; n, and A is singular (non-invertible).\n\nHere’s the updated section for your Quarto study sheet with the new problems and their solutions:"
  },
  {
    "objectID": "general_topics/rank.html#practice-problems",
    "href": "general_topics/rank.html#practice-problems",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "For a 6 \\times 3 matrix A with rank 3, determine:\n\n\\text{dim Nul}(A)\n\\text{dim Row}(A)\n\\text{rank}(A^T)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\text{dim Nul}(A) = 0 since \\text{rank}(A) = 3.\n\\text{dim Row}(A) = 3.\n\\text{rank}(A^T) = 3, as rank remains the same under transposition.\n\n\n\n\n\n\n\nIf the null space of a 7 \\times 6 matrix A is 5-dimensional, what is \\text{dim Col}(A)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the Rank-Nullity Theorem: text{rank}(A) + \\text{dim Nul}(A) = 6 Since \\text{dim Nul}(A) = 5, \\text{rank}(A) = 1, so \\text{dim Col}(A) = 1.\n\n\n\n\n\n\nIf A is a 6 \\times 4 matrix, what is the smallest possible dimension of Nul(A)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe smallest possible dimension of Nul(A) is 0 if \\text{rank}(A) = 4, meaning the matrix has full column rank.\n\n\n\n\n\n\nSuppose the solutions of a homogeneous system of five linear equations in six unknowns are all multiples of one nonzero solution. Will the system necessarily have a solution for every possible choice of constants on the right sides of the equations? Explain.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe system does not necessarily have a solution for every possible choice of constants. The key observation is that the homogeneous system has a null space of dimension 1, as all solutions are multiples of one nonzero solution. For the inhomogeneous system to have a solution for every choice of constants, the coefficient matrix must have full row rank (rank 5). However, since there are 6 unknowns, the rank cannot exceed 5, leaving one degree of freedom in the solution space. This means there may exist some choices of constants for which the system is inconsistent.\n\n\n\n\n\n\nIn statistical theory, a common requirement is that a matrix be of full rank. That is, the rank should be as large as possible. Explain why an m \\times n matrix with more rows than columns has full rank if and only if its columns are linearly independent.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAn m \\times n matrix A (where m &gt; n) has full rank when \\text{rank}(A) = n, which implies that the columns span an n-dimensional space. This is only possible if the columns are linearly independent; otherwise, the rank would be less than n. If the columns are linearly dependent, at least one column can be expressed as a linear combination of others, reducing the rank. Therefore, full rank requires that the columns be linearly independent."
  },
  {
    "objectID": "general_topics/rank.html#additional-resources",
    "href": "general_topics/rank.html#additional-resources",
    "title": "Rank in Linear Algebra",
    "section": "",
    "text": "3Blue1Brown - Essence of Linear Algebra (YouTube)\nKhan Academy - Linear Algebra (YouTube)\nMIT OpenCourseWare - Linear Algebra"
  },
  {
    "objectID": "exam3/semi_daily_review.html",
    "href": "exam3/semi_daily_review.html",
    "title": "Important Applications (Review Every Other Day)",
    "section": "",
    "text": "Note\n\n\n\nDefinition: The determinant of a square matrix A is a scalar value denoted \\det(A) or |A| that provides key insights into the matrix’s properties, such as invertibility and volume scaling.\n\n\n\nFor a 2 \\times 2 matrix: \\det \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\nProperties:\n\n\\det(I) = 1 for the identity matrix I.\nIf \\det(A) = 0, the matrix A is singular (non-invertible).\nDeterminants are multiplicative: \\det(AB) = \\det(A)\\det(B).\nTranspose invariance: \\det(A^T) = \\det(A).\n\n\n\n\n\nDeterminants help calculate:\n\nArea or Volume Scaling:\n\nFor 2 \\times 2: The absolute value of the determinant gives the area of the parallelogram spanned by the column vectors of the matrix.\nFor 3 \\times 3: It represents the volume of the parallelepiped.\n\nOrientation: The sign of \\det(A) indicates whether a transformation preserves or reverses orientation.\n\n\n\nFind the area of the parallelogram formed by vectors \\mathbf{u} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe determinant of the matrix A = [\\mathbf{u} \\ \\mathbf{v}] is:\n\\det(A) = \\det\\begin{bmatrix} 3 & 2 \\\\ 1 & 4 \\end{bmatrix} = (3)(4) - (1)(2) = 12 - 2 = 10\nArea = |\\det(A)| = 10.\n\n\n\n\n\n\n\n\n\nA matrix A is invertible if \\det(A) \\neq 0. For 2 \\times 2 matrices, the inverse can be calculated as:\nA^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\n\n\nFind the inverse of A = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nCompute \\det(A):\n\n\\det(A) = (2)(4) - (1)(3) = 8 - 3 = 5\n\nUse the formula:\n\nA^{-1} = \\frac{1}{5} \\begin{bmatrix} 4 & -3 \\\\ -1 & 2 \\end{bmatrix} = \\begin{bmatrix} 0.8 & -0.6 \\\\ -0.2 & 0.4 \\end{bmatrix}\n\n\n\n\n\n\nThe determinant is used in finding eigenvalues \\lambda of A by solving:\n\\det(A - \\lambda I) = 0\n\n\n\nLet A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}. Find its eigenvalues.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nCompute \\det(A - \\lambda I):\n\n\\det(A - \\lambda I) = \\det\\begin{bmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda\\end{bmatrix} = (4-\\lambda)(3-\\lambda) - 2 2. Expand and solve:\n\\lambda^2 - 7\\lambda + 10 = 0\n\nEigenvalues: \\lambda = 5, 2."
  },
  {
    "objectID": "exam3/semi_daily_review.html#definition-and-basic-properties",
    "href": "exam3/semi_daily_review.html#definition-and-basic-properties",
    "title": "Important Applications (Review Every Other Day)",
    "section": "",
    "text": "Note\n\n\n\nDefinition: The determinant of a square matrix A is a scalar value denoted \\det(A) or |A| that provides key insights into the matrix’s properties, such as invertibility and volume scaling.\n\n\n\nFor a 2 \\times 2 matrix: \\det \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\nProperties:\n\n\\det(I) = 1 for the identity matrix I.\nIf \\det(A) = 0, the matrix A is singular (non-invertible).\nDeterminants are multiplicative: \\det(AB) = \\det(A)\\det(B).\nTranspose invariance: \\det(A^T) = \\det(A)."
  },
  {
    "objectID": "exam3/semi_daily_review.html#geometric-interpretation",
    "href": "exam3/semi_daily_review.html#geometric-interpretation",
    "title": "Important Applications (Review Every Other Day)",
    "section": "",
    "text": "Determinants help calculate:\n\nArea or Volume Scaling:\n\nFor 2 \\times 2: The absolute value of the determinant gives the area of the parallelogram spanned by the column vectors of the matrix.\nFor 3 \\times 3: It represents the volume of the parallelepiped.\n\nOrientation: The sign of \\det(A) indicates whether a transformation preserves or reverses orientation.\n\n\n\nFind the area of the parallelogram formed by vectors \\mathbf{u} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} and \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe determinant of the matrix A = [\\mathbf{u} \\ \\mathbf{v}] is:\n\\det(A) = \\det\\begin{bmatrix} 3 & 2 \\\\ 1 & 4 \\end{bmatrix} = (3)(4) - (1)(2) = 12 - 2 = 10\nArea = |\\det(A)| = 10."
  },
  {
    "objectID": "exam3/semi_daily_review.html#applications-in-linear-algebra",
    "href": "exam3/semi_daily_review.html#applications-in-linear-algebra",
    "title": "Important Applications (Review Every Other Day)",
    "section": "",
    "text": "A matrix A is invertible if \\det(A) \\neq 0. For 2 \\times 2 matrices, the inverse can be calculated as:\nA^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\n\n\nFind the inverse of A = \\begin{bmatrix} 2 & 3 \\\\ 1 & 4 \\end{bmatrix}.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nCompute \\det(A):\n\n\\det(A) = (2)(4) - (1)(3) = 8 - 3 = 5\n\nUse the formula:\n\nA^{-1} = \\frac{1}{5} \\begin{bmatrix} 4 & -3 \\\\ -1 & 2 \\end{bmatrix} = \\begin{bmatrix} 0.8 & -0.6 \\\\ -0.2 & 0.4 \\end{bmatrix}\n\n\n\n\n\n\nThe determinant is used in finding eigenvalues \\lambda of A by solving:\n\\det(A - \\lambda I) = 0\n\n\n\nLet A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}. Find its eigenvalues.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nCompute \\det(A - \\lambda I):\n\n\\det(A - \\lambda I) = \\det\\begin{bmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda\\end{bmatrix} = (4-\\lambda)(3-\\lambda) - 2 2. Expand and solve:\n\\lambda^2 - 7\\lambda + 10 = 0\n\nEigenvalues: \\lambda = 5, 2."
  },
  {
    "objectID": "exam4/chapter6_5_v2.html",
    "href": "exam4/chapter6_5_v2.html",
    "title": "Chapter 6.5",
    "section": "",
    "text": "If A is m \\times n and b is in \\mathbb{R}^m, a least-squares solution of A\\mathbf{x}=\\mathbf{b} is an \\hat{\\mathbf{x}} in \\mathbb{R}^n such that:\n\n\\|\\mathbf{b}-A\\hat{\\mathbf{x}}\\| \\le \\|\\mathbf{b} - A\\mathbf{x}\\|\n\nfor all x in \\mathbb{R}^n.\n\n\n\n\n\n\n\n\n\n\n\n\nGiven a system of linear equations A\\mathbf{x} = \\mathbf{b}, the normal equations are given by:\n\nA^TA\\mathbf{x} = A^T\\mathbf{b}\n\nwhere A^T is the transpose of matrix A."
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#least-squares-problems",
    "href": "exam4/chapter6_5_v2.html#least-squares-problems",
    "title": "Chapter 6.5",
    "section": "",
    "text": "If A is m \\times n and b is in \\mathbb{R}^m, a least-squares solution of A\\mathbf{x}=\\mathbf{b} is an \\hat{\\mathbf{x}} in \\mathbb{R}^n such that:\n\n\\|\\mathbf{b}-A\\hat{\\mathbf{x}}\\| \\le \\|\\mathbf{b} - A\\mathbf{x}\\|\n\nfor all x in \\mathbb{R}^n."
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#normal-equations",
    "href": "exam4/chapter6_5_v2.html#normal-equations",
    "title": "Chapter 6.5",
    "section": "",
    "text": "Given a system of linear equations A\\mathbf{x} = \\mathbf{b}, the normal equations are given by:\n\nA^TA\\mathbf{x} = A^T\\mathbf{b}\n\nwhere A^T is the transpose of matrix A."
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#theorem-13-the-least-squares-theorem",
    "href": "exam4/chapter6_5_v2.html#theorem-13-the-least-squares-theorem",
    "title": "Chapter 6.5",
    "section": "2.1 Theorem 13: The Least-Squares Theorem",
    "text": "2.1 Theorem 13: The Least-Squares Theorem\n\n\n\n\n\n\nStatement: The set of least-squares solutions of A\\mathbf{x} = \\mathbf{b} coincides with the nonempty set of solutions of the normal equations A^TA\\mathbf{x} = A^T\\mathbf{b}.\nLayman’s Explanation: Imagine you have a system of linear equations, A\\mathbf{x} = \\mathbf{b}, that might not have an exact solution. This often happens when you have more equations than unknowns (an overdetermined system). In such cases, you can’t find an \\mathbf{x} that perfectly satisfies the equation.\nThe “least-squares solution” is the best approximation you can get. It’s the \\mathbf{x} that minimizes the distance between A\\mathbf{x} and \\mathbf{b}. Think of it like trying to fit a line to a set of points that don’t perfectly align. The least-squares line is the one that comes closest to all the points, minimizing the overall error.\nThis theorem says that finding this best approximation (the least-squares solution) is the same as solving a different set of equations called the “normal equations,” which are given by A^TA\\mathbf{x} = A^T\\mathbf{b}. Importantly, the normal equations always have at least one solution, even if the original system doesn’t.\nExample: Let’s say you’re trying to find the best-fit line y = mx + c for the following data points: (1, 2), (2, 3), (3, 5).\nYou can represent this as a system of equations:\n\n\\begin{aligned}\nm + c &= 2 \\\\\n2m + c &= 3 \\\\\n3m + c &= 5\n\\end{aligned}\n\nThis can be written in matrix form as A\\mathbf{x} = \\mathbf{b}, where:\nA = \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix}, \\mathbf{x} = \\begin{bmatrix} m \\\\ c \\end{bmatrix}, and \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}.\nThis system has no exact solution (the points don’t lie on a perfect line). To find the least-squares solution (the best-fit line), you would solve the normal equations:\nA^TA\\mathbf{x} = A^T\\mathbf{b}\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix} \\begin{bmatrix} m \\\\ c \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}\nSolving this system will give you the values of m and c that define the best-fit line.\nProof Explanation: The proof has two main parts:\n\nShowing that a least-squares solution satisfies the normal equations: It’s already been shown (in a previous part of the text, referred to as “above”) that if \\hat{\\mathbf{x}} is a least-squares solution, then it must satisfy the normal equations A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}.\nShowing that a solution to the normal equations is a least-squares solution: Now, let’s assume \\hat{\\mathbf{x}} satisfies the normal equations: A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}.\n\nThis implies that A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = \\mathbf{0}.\nThis means that the vector \\mathbf{b} - A\\hat{\\mathbf{x}} is orthogonal (perpendicular) to the rows of A^T, and therefore orthogonal to the columns of A.\nSince the columns of A span the column space of A (denoted as Col A), the vector \\mathbf{b} - A\\hat{\\mathbf{x}} is orthogonal to every vector in Col A.\nWe can decompose \\mathbf{b} into two parts: \\mathbf{b} = A\\hat{\\mathbf{x}} + (\\mathbf{b} - A\\hat{\\mathbf{x}}). The first part, A\\hat{\\mathbf{x}}, lies in Col A, and the second part, (\\mathbf{b} - A\\hat{\\mathbf{x}}), is orthogonal to Col A.\nBy the Pythagorean Theorem, \\|\\mathbf{b}\\|^2 = \\|A\\hat{\\mathbf{x}}\\|^2 + \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|^2.\nFor any other vector \\mathbf{x} in \\mathbb{R}^n, \\mathbf{b} - A\\mathbf{x} can also be decomposed into a part in Col A and a part orthogonal to Col A.\nAgain, by the Pythagorean Theorem, \\|\\mathbf{b}\\|^2 = \\|A\\mathbf{x}\\|^2 + \\|\\mathbf{b} - A\\mathbf{x}\\|^2.\nSince \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|^2 is the squared distance from \\mathbf{b} to Col A, and this distance is minimized when \\mathbf{x} = \\hat{\\mathbf{x}}, we have \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\|^2 \\le \\|\\mathbf{b} - A\\mathbf{x}\\|^2.\nTherefore, \\hat{\\mathbf{x}} is a least-squares solution."
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#theorem-14",
    "href": "exam4/chapter6_5_v2.html#theorem-14",
    "title": "Chapter 6.5",
    "section": "2.2 Theorem 14",
    "text": "2.2 Theorem 14\n\n\n\n\n\n\nLet A be an m \\times n matrix. The following statements are logically equivalent:\n\nThe equation A\\mathbf{x} = \\mathbf{b} has a unique least-squares solution for each \\mathbf{b} \\in \\mathbb{R}^m.\nThe columns of A are linearly independent.\nThe matrix A^T A is invertible.\n\nWhen these statements are true, the least-squares solution \\hat{\\mathbf{x}} is given by:\n\n\\hat{\\mathbf{x}} = (A^T A)^{-1} A^T \\mathbf{b}"
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#theorem-15",
    "href": "exam4/chapter6_5_v2.html#theorem-15",
    "title": "Chapter 6.5",
    "section": "2.3 Theorem 15",
    "text": "2.3 Theorem 15\n\n\n\n\n\n\nGiven an m \\times n matrix A with linearly independent columns, let A = QR be a QR factorization of A (as defined in Theorem 12). For every \\mathbf{b} in \\mathbb{R}^m, the equation A\\mathbf{x} = \\mathbf{b} has a unique least-squares solution, which is given by:\n\n\\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}\n\nProof (in simple terms)\n\nStart with the formula for \\hat{\\mathbf{x}}: \n\\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}\n\nSubstitute A = QR into the equation A\\mathbf{x}: \nA\\hat{\\mathbf{x}} = QR\\hat{\\mathbf{x}}\n\nReplace \\hat{\\mathbf{x}} with R^{-1}Q^T\\mathbf{b}: \nA\\hat{\\mathbf{x}} = QRR^{-1}Q^T\\mathbf{b}\n\nSimplify RR^{-1} (which equals the identity matrix I): \nA\\hat{\\mathbf{x}} = QQ^T\\mathbf{b}\n\nBy Theorem 12, the columns of Q form an orthonormal basis for the column space of A. Therefore, QQ^T\\mathbf{b} is the projection of \\mathbf{b} onto the column space of A.\nThis shows that \\hat{\\mathbf{x}} minimizes the error (i.e., the difference between A\\mathbf{x} and \\mathbf{b}), meaning \\hat{\\mathbf{x}} is the least-squares solution.\nSince R is invertible and Q is orthonormal, \\hat{\\mathbf{x}} is unique.\n\nNumerical Note\n\nSince R is an upper triangular matrix, it’s computationally efficient to solve the equation: \nR\\mathbf{x} = Q^T\\mathbf{b}\n\nThis avoids directly calculating R^{-1}, which can be slow and error-prone. Instead, use back-substitution or row operations to solve for \\mathbf{x} efficiently."
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#example-1-finding-the-least-squares-solution",
    "href": "exam4/chapter6_5_v2.html#example-1-finding-the-least-squares-solution",
    "title": "Chapter 6.5",
    "section": "3.1 Example 1: Finding the Least-Squares Solution",
    "text": "3.1 Example 1: Finding the Least-Squares Solution\nFind the least-squares solution of the inconsistent system A\\mathbf{x} = \\mathbf{b} for\nA = \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}.\n\n3.1.1 Solution\n\nCalculate A^TA and A^T\\mathbf{b}:\nA^TA = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{bmatrix} = \\begin{bmatrix} 14 & 6 \\\\ 6 & 3 \\end{bmatrix}\nA^T\\mathbf{b} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 23 \\\\ 10 \\end{bmatrix}\nSolve the normal equations A^TA\\mathbf{x} = A^T\\mathbf{b}:\n\\begin{bmatrix} 14 & 6 \\\\ 6 & 3 \\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 23 \\\\ 10 \\end{bmatrix}\nSolving this system gives \\mathbf{x} = \\begin{bmatrix} 1.3 \\\\ 0.4 \\end{bmatrix}.\n\n\n\nShow the code\nimport numpy as np\n\n# Define A and b\nA = np.array([[1, 1], [2, 1], [3, 1]])\nb = np.array([2, 3, 5])\n\n# Calculate A^T * A and A^T * b\nATA = A.T @ A\nATb = A.T @ b\n\n# Solve the normal equations\nx = np.linalg.solve(ATA, ATb)\n\nprint(\"Least-squares solution:\", x)\n\n\nLeast-squares solution: [1.5        0.33333333]"
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#example-2-using-qr-factorization",
    "href": "exam4/chapter6_5_v2.html#example-2-using-qr-factorization",
    "title": "Chapter 6.5",
    "section": "3.2 Example 2: Using QR Factorization",
    "text": "3.2 Example 2: Using QR Factorization\nFind the least-squares solution of the system in Example 1 using a QR factorization of A.\n\n3.2.1 Solution\n\nFind the QR factorization of A:\nUsing the Gram-Schmidt process (or a software package), we can find A = QR, where\nQ = \\begin{bmatrix} 1/\\sqrt{14} & 1/\\sqrt{3} \\\\ 2/\\sqrt{14} & 0 \\\\ 3/\\sqrt{14} & -1/\\sqrt{3} \\end{bmatrix}, R = \\begin{bmatrix} \\sqrt{14} & \\sqrt{14}/\\sqrt{3} \\\\ 0 & \\sqrt{3} \\end{bmatrix}.\nCalculate Q^T\\mathbf{b}:\nQ^T\\mathbf{b} = \\begin{bmatrix} 1/\\sqrt{14} & 2/\\sqrt{14} & 3/\\sqrt{14} \\\\ 1/\\sqrt{3} & 0 & -1/\\sqrt{3} \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 23/\\sqrt{14} \\\\ -3/\\sqrt{3} \\end{bmatrix}\nSolve R\\mathbf{x} = Q^T\\mathbf{b}:\n\\begin{bmatrix} \\sqrt{14} & \\sqrt{14}/\\sqrt{3} \\\\ 0 & \\sqrt{3} \\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} 23/\\sqrt{14} \\\\ -3/\\sqrt{3} \\end{bmatrix}\nSolving this system (using back-substitution) gives \\mathbf{x} = \\begin{bmatrix} 1.3 \\\\ 0.4 \\end{bmatrix}, which matches the solution from Example 1.\n\n\n\nShow the code\nimport numpy as np\n\n# Define A and b\nA = np.array([[1, 1], [2, 1], [3, 1]])\nb = np.array([2, 3, 5])\n\n# Calculate QR factorization\nQ, R = np.linalg.qr(A)\n\n# Calculate Q^T * b\nQTb = Q.T @ b\n\n# Solve Rx = Q^T * b\nx = np.linalg.solve(R, QTb)\n\nprint(\"Least-squares solution:\", x)\n\n\nLeast-squares solution: [1.5        0.33333333]"
  },
  {
    "objectID": "exam4/chapter6_5_v2.html#visualizing-the-least-squares-solution",
    "href": "exam4/chapter6_5_v2.html#visualizing-the-least-squares-solution",
    "title": "Chapter 6.5",
    "section": "4.1 Visualizing the Least-Squares Solution",
    "text": "4.1 Visualizing the Least-Squares Solution\nThe following plot shows the data points from Example 1 and the least-squares line:\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data points\nx_data = np.array([1, 2, 3])\ny_data = np.array([2, 3, 5])\n\n# Least-squares solution from Example 1\nm = 1.3\nc = 0.4\n\n# Generate points for the line\nx_line = np.linspace(0, 4, 100)\ny_line = m * x_line + c\n\n# Plot the data and the line\nplt.scatter(x_data, y_data, label=\"Data points\")\nplt.plot(x_line, y_line, label=\"Least-squares line\", color=\"red\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Least-Squares Line\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nExplanation:\n\nThe blue dots represent the original data points.\nThe red line is the least-squares line, which is the best-fit line that minimizes the sum of the squared vertical distances between the points and the line."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "Linear Algebra –&gt; Vector Spaces\n\n\n\n\n\n\n\ngraph TD\n    A[Linear Algebra] --&gt; B[Vector Spaces]\n\n    B --&gt; B1[Definition]\n    B1 --&gt; B1a[Set with addition and scalar multiplication]\n\n    B --&gt; B2[Vectors]\n    B2 --&gt; B2a[Linear Independence]\n    B2 --&gt; B2b[Linear Dependence]\n    B2 --&gt; B2c[Linear Combinations]\n\n    B --&gt; B3[Basis]\n    B3 --&gt; B3a[Linearly independent set that spans space]\n    B3 --&gt; B3b[Dimension]\n    B3b --&gt; B3b1[Number of vectors in any basis]\n    \n    click B3a \"linear_independence_and_span.html\"\n    click B3b \"vector_spaces_and_dimensions.html\"\n\nstyle A fill:#f9f,stroke:#333,stroke-width:4px\nstyle B fill:#bbf,stroke:#333,stroke-width:2px\n\n\n\n\n\nFigure 1: Interactive diagram with clickable nodes\n\n\n\n\n\n\n\n\nLinear Algebra –&gt; Matrices\n\n\n\n\n\n\ngraph TD\n    A[Linear Algebra] --&gt; C[Matrices]\n    C --&gt; C1[Row Space]\n    C1 --&gt; C1a[Span of row vectors]\n    C1 --&gt; C1b[dim = row rank]\n\n    C --&gt; C2[Column Space]\n    C2 --&gt; C2a[Span of column vectors]\n    C2 --&gt; C2b[dim = column rank]\n\n    C --&gt; C3[Null Space]\n    C3 --&gt; C3a[Solutions to Ax = 0]\n    C3 --&gt; C3b[dim = nullity]\n\n    C --&gt; C4[Pivots]\n    C4 --&gt; C4a[Leading non-zero entries]\n    C4 --&gt; C4b[Number of pivots = rank]\n\nstyle A fill:#f9f,stroke:#333,stroke-width:4px\nstyle C,D fill:#bbf,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\nLinear Algebra –&gt; Fundamental Theorems\n\n\n\n\n\n\ngraph TD\n    A[Linear Algebra] --&gt; D[Fundamental Theorems]\n\n    D --&gt; D1[Rank-Nullity Theorem]\n    D1 --&gt; D1a[rank + nullity = number of columns]\n\n    D --&gt; D2[Row-Column Rank]\n    D2 --&gt; D2a[row rank = column rank]\n\n    D --&gt; D3[Basis Theorem]\n    D3 --&gt; D3a[All bases have same size]\n\nstyle A fill:#f9f,stroke:#333,stroke-width:4px\nstyle D fill:#bbf,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension = Rank = Size\n\n\n\n\nDimension of a vector space = Number of vectors in any basis\nRank of a matrix = Number of pivot columns\nSize of a span = Number of linearly independent vectors\n\n\n\n\n\n\n\n\n\nSpan = Range = Image\n\n\n\n\nSpan of vectors = All possible linear combinations\nRange of a transformation = Output vectors\nImage of a matrix = Column space\n\n\n\n\n\n\n\n\n\nKernel = Null Space = Solution Space\n\n\n\n\nKernel of transformation = Vectors that map to zero\nNull space of matrix = Solutions to A\\mathbf{x} = \\mathbf{0}\nSolution space of homogeneous system = Null space\n\n\n\n\n\n\n\n\n\nTrace = Sum Diagonal Elements\n\n\n\n\nTrace = The Sum of all Diagonal Elements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA vector space is a set with two operations: 1. Vector addition 2. Scalar multiplication\nThat satisfy certain axioms (closure, associativity, etc.)\n\n\n\n\n\nEvery vector space has a basis\nDimension = Number of vectors in any basis\nSubspaces inherit properties from parent space\n\\dim(\\text{subspace}) \\leq \\dim(\\text{parent space})\n\n\n\n\n\n\nRow SpaceColumn SpaceNull Space\n\n\n\nSpan of row vectors\n\\dim(\\text{row space}) = \\text{row rank}\nRow operations don’t change row space\n\n\n\n\nSpan of column vectors\n\\dim(\\text{column space}) = \\text{column rank}\nImage of the matrix transformation\n\n\n\n\nSolutions to A\\mathbf{x} = \\mathbf{0}\n\\dim(\\text{null space}) = \\text{nullity}\nKernel of the matrix transformation\n\n\n\n\n\n\n\nRow rank = Column rank = Rank of matrix\nNumber of pivot columns = Rank\nNumber of free variables = Nullity\n\n\n\n\n\n\n\n\n\n\n\nRank-Nullity Theorem\n\n\n\nFor an m \\times n matrix: \\text{rank} + \\text{nullity} = n (number of columns)\nImplications: - More columns than rows → guaranteed null space - Full column rank → trivial null space\n\n\n\n\n\n\n\n\nRow-Column Rank Theorem\n\n\n\nRow rank = Column rank\nImplications: - Can find rank by counting pivot rows OR pivot columns - Maximum rank is min(rows, columns)\n\n\n\n\n\n\n\n\ndim(row space) ──────┐\n                     ├── all equal ── RANK\ndim(column space) ───┘\n\nRANK + NULLITY = NUMBER OF COLUMNS\n\n\n\nBASIS ─── consists of ─── LINEARLY INDEPENDENT vectors\n   │                           │\n   └── that ─── SPANS ────────┘\n   │\n   └── number of vectors = DIMENSION\n\n\n\n\n\n\n\n\n\n\nImportant Distinctions\n\n\n\n\nLinear independence ≠ Spanning\n\nA set can span without being linearly independent\nA set can be linearly independent without spanning\n\nRank ≠ Number of Rows/Columns\n\nRank is ≤ min(rows, columns)\nEqual only for full rank matrices\n\nZero vector ≠ Empty set\n\nZero vector is IN every vector space\nEmpty set is NOT a vector space\n\n\n\n\n\n\n\n\n\n\nA matrix A is invertible if there exists a matrix B such that AB = BA = I, where I is the identity matrix.\nThe determinant of a matrix, denoted as det A, plays a crucial role in determining whether a matrix is invertible or not.\nIf det A ≠ 0, then the matrix A is invertible. If det A = 0, then the matrix A is not invertible.\nInvertible matrices represent linear transformations that are one-to-one and onto (bijective), while non-invertible matrices represent linear transformations that are not one-to-one or not onto.\n\n\n\n\n\nThe determinant of a square matrix, det A, is a scalar value that reflects various properties of the matrix.\ndet A = 0 if and only if the columns (or rows) of the matrix are linearly dependent, meaning they do not form a basis for the vector space.\nThe determinant of a matrix also represents the scaling factor of the volume (or area in 2D) of the parallelotope (or parallelogram in 2D) formed by the columns (or rows) of the matrix.\nIf det A &gt; 0, the linear transformation represented by the matrix preserves the orientation of the vector space. If det A &lt; 0, the linear transformation reverses the orientation.\n\n\n\n\n\nA linear transformation T: V → W is one-to-one (injective) if for every y in W, there is at most one x in V such that T(x) = y.\nA linear transformation T: V → W is many-to-one (surjective) if for every y in W, there is at least one x in V such that T(x) = y.\nIf a linear transformation T: V → W is one-to-one and onto (bijective), then it has an inverse transformation T^(-1): W → V, and the matrix representation of T^(-1) is the inverse of the matrix representation of T.\nOne-to-one transformations preserve the linear independence of vectors, while many-to-one transformations may map linearly independent vectors to linearly dependent vectors.\n\n\n\n\n\n\nBack to Core Concepts\nJump to Theorems\nSee Matrix Spaces"
  },
  {
    "objectID": "index.html#quick-reference-equivalent-terms",
    "href": "index.html#quick-reference-equivalent-terms",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "Dimension = Rank = Size\n\n\n\n\nDimension of a vector space = Number of vectors in any basis\nRank of a matrix = Number of pivot columns\nSize of a span = Number of linearly independent vectors\n\n\n\n\n\n\n\n\n\nSpan = Range = Image\n\n\n\n\nSpan of vectors = All possible linear combinations\nRange of a transformation = Output vectors\nImage of a matrix = Column space\n\n\n\n\n\n\n\n\n\nKernel = Null Space = Solution Space\n\n\n\n\nKernel of transformation = Vectors that map to zero\nNull space of matrix = Solutions to A\\mathbf{x} = \\mathbf{0}\nSolution space of homogeneous system = Null space\n\n\n\n\n\n\n\n\n\nTrace = Sum Diagonal Elements\n\n\n\n\nTrace = The Sum of all Diagonal Elements"
  },
  {
    "objectID": "index.html#core-concepts-and-their-relationships",
    "href": "index.html#core-concepts-and-their-relationships",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "Important\n\n\n\nA vector space is a set with two operations: 1. Vector addition 2. Scalar multiplication\nThat satisfy certain axioms (closure, associativity, etc.)\n\n\n\n\n\nEvery vector space has a basis\nDimension = Number of vectors in any basis\nSubspaces inherit properties from parent space\n\\dim(\\text{subspace}) \\leq \\dim(\\text{parent space})\n\n\n\n\n\n\nRow SpaceColumn SpaceNull Space\n\n\n\nSpan of row vectors\n\\dim(\\text{row space}) = \\text{row rank}\nRow operations don’t change row space\n\n\n\n\nSpan of column vectors\n\\dim(\\text{column space}) = \\text{column rank}\nImage of the matrix transformation\n\n\n\n\nSolutions to A\\mathbf{x} = \\mathbf{0}\n\\dim(\\text{null space}) = \\text{nullity}\nKernel of the matrix transformation\n\n\n\n\n\n\n\nRow rank = Column rank = Rank of matrix\nNumber of pivot columns = Rank\nNumber of free variables = Nullity\n\n\n\n\n\n\n\n\n\n\n\nRank-Nullity Theorem\n\n\n\nFor an m \\times n matrix: \\text{rank} + \\text{nullity} = n (number of columns)\nImplications: - More columns than rows → guaranteed null space - Full column rank → trivial null space\n\n\n\n\n\n\n\n\nRow-Column Rank Theorem\n\n\n\nRow rank = Column rank\nImplications: - Can find rank by counting pivot rows OR pivot columns - Maximum rank is min(rows, columns)"
  },
  {
    "objectID": "index.html#visual-summary-of-relationships",
    "href": "index.html#visual-summary-of-relationships",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "dim(row space) ──────┐\n                     ├── all equal ── RANK\ndim(column space) ───┘\n\nRANK + NULLITY = NUMBER OF COLUMNS\n\n\n\nBASIS ─── consists of ─── LINEARLY INDEPENDENT vectors\n   │                           │\n   └── that ─── SPANS ────────┘\n   │\n   └── number of vectors = DIMENSION"
  },
  {
    "objectID": "index.html#common-misconceptions-and-clarifications",
    "href": "index.html#common-misconceptions-and-clarifications",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "Important Distinctions\n\n\n\n\nLinear independence ≠ Spanning\n\nA set can span without being linearly independent\nA set can be linearly independent without spanning\n\nRank ≠ Number of Rows/Columns\n\nRank is ≤ min(rows, columns)\nEqual only for full rank matrices\n\nZero vector ≠ Empty set\n\nZero vector is IN every vector space\nEmpty set is NOT a vector space"
  },
  {
    "objectID": "index.html#additional-concepts-and-relationships",
    "href": "index.html#additional-concepts-and-relationships",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "A matrix A is invertible if there exists a matrix B such that AB = BA = I, where I is the identity matrix.\nThe determinant of a matrix, denoted as det A, plays a crucial role in determining whether a matrix is invertible or not.\nIf det A ≠ 0, then the matrix A is invertible. If det A = 0, then the matrix A is not invertible.\nInvertible matrices represent linear transformations that are one-to-one and onto (bijective), while non-invertible matrices represent linear transformations that are not one-to-one or not onto.\n\n\n\n\n\nThe determinant of a square matrix, det A, is a scalar value that reflects various properties of the matrix.\ndet A = 0 if and only if the columns (or rows) of the matrix are linearly dependent, meaning they do not form a basis for the vector space.\nThe determinant of a matrix also represents the scaling factor of the volume (or area in 2D) of the parallelotope (or parallelogram in 2D) formed by the columns (or rows) of the matrix.\nIf det A &gt; 0, the linear transformation represented by the matrix preserves the orientation of the vector space. If det A &lt; 0, the linear transformation reverses the orientation.\n\n\n\n\n\nA linear transformation T: V → W is one-to-one (injective) if for every y in W, there is at most one x in V such that T(x) = y.\nA linear transformation T: V → W is many-to-one (surjective) if for every y in W, there is at least one x in V such that T(x) = y.\nIf a linear transformation T: V → W is one-to-one and onto (bijective), then it has an inverse transformation T^(-1): W → V, and the matrix representation of T^(-1) is the inverse of the matrix representation of T.\nOne-to-one transformations preserve the linear independence of vectors, while many-to-one transformations may map linearly independent vectors to linearly dependent vectors."
  },
  {
    "objectID": "index.html#navigation-links",
    "href": "index.html#navigation-links",
    "title": "Linear Algebra Study Book",
    "section": "",
    "text": "Back to Core Concepts\nJump to Theorems\nSee Matrix Spaces"
  }
]